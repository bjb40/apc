---author:	- name: Bryce Bartlett	- name: Stephen Vaiseytitle: Ensembling Age-Period-Cohort Estimates by Averaging Multiple Window Constraint Models using Markov Chain Monte Carlo Methods bibliography: citations/apc.bibcsl: citations/asa-mod.csl---#AbstractKey words: Computational Sociology, Methods, Life Course, Bayesian, Culture#Introduction#BackgroundBoth classic and modern studies have proposed different effects across three social dimensions of time: age, period, and cohort [@ryder_cohort_1965]. These dimensions of time operate on individuals in different ways as they flow through social space. Age effects are driven either by the common experiences associated with the biological process of aging [@jackson_biological_2003], or persistent age-structuring institutions, like high school or retirement [@mortimer_government_2003; @waite_constrained_2014].  Period effects are responses of everyone to contemporaneous social experiences, like recessions or wars [@lam_is_2014]. Finally, cohort effects are socialization effects. They are broad-based historical events that stick to the populations experiencing them, even after the event has long since ended [@vaisey_cultural_2016]. In contemporary research, cohorts are virtually always defined in terms of birth year.These three dimensions of time sit at the center of some recurrent debates. For example, cultural sociologists are divided over whether the main driver of culture is a period process (cultural fragmentation) or a cohort process (acquired dispositions) [@vaisey_cultural_2016]. Clinical reasearchers and biodemographers are looking for biological age---indicators of biological deterioration---but argue about confounding from cohort changes [@jackson_biological_2003]. And yet others are concerned about separating long-term and short-term impacts of important events, like the Great Recession [@burgard_effects_2015].Unfortunately, it is not trivial to statistically estimate the unique effects of age, period, and cohort which would allow analysts to evaluate evidence that may settle these questions. The fundamental problem is identification: these three dimensions are linearly dependent. Two of the dimensions define the third. If a researcher knows an individual's age, and the year of the survey, cohort is also defined. Because these variables are exactly colinear, they are not estimable using classic statistical techniques. A number of solutions have been proposed to address this conundrum, these include traditional methods like block/window constraints [@glenn_strategies_2005], and new methods, including statistical transformation (the "intrinsic estimator"), and random effects models [@yang_age-period-cohort_2013]. The past several years have seen a resurgence in debates surrounding on methodology [@luo_block_2016; @luo_assessing_2013; @bell_hierarchical_2017]. Perhaps the most common argument is that the "constraints", *i.e.* assumptions used to break the APC identity, produce unknown biases into the models. These particular complaints represent the extreme case of multicollinearity, a well-known, though difficult problem in statistical theory [@wooldridge_introductory_2009, at p. 95-98], with numerous cautions in applied texts that separating highly colinear effects is difficult [@camm_essentials_2016, at p. 328]. Viewing the APC identity problem as an issue of multicolinnearity in a Bayesian perspective provides a simple way to understand and adcress the problem. Ultimately, multicollinearity significantly reduce the ability of the data to supply information to produce the estimates, potentially introducing sensitivity to the prior [@gelman_bayesian_2014, at p. 305-306], including assumtpions of linearity. To put it another way, the critique of APC models in general, and window constraints in particular [@luo_block_2016], is that different sets of assumptions lead to different (and inconsistent) results. But, what if instead of focusing on finding a *best* fitting model, we use a simple nonlinear approach (block or window modelling), and hundreds of assumptions? Ensembling is the combination of estimates from different models subject to some weighting scheme. Bayesian Model Averaging (BMA) allows us to do just that with the APC problem: estimate hundreds---perhaps even thousands---of models with differeing assumptions, and then combine them, weighted by a measure of the model's posterior probability.#Bayesian Model AveragingThe theoretical backdrop of BMA applies to this curcumstance quite well. In principal, there is no one true (or best) model; instead estimates are conditional on models from the modeling space ($\mathscr{M}$), and have  a posterior distribution, which is calculated as a weighted average of all models [@raftery_bayesian_1995, p. 144-145].  It has been applied in diverse areas from weather forecasting to biology to social science [@fragoso_bayesian_2015] . BMA operates under the simple fact that any particular estimate, including effect size and significance, have a posterior probability distribution which is calculated as "an average of the posterior distributions under each of the models considered, weighted by their posterior model probability." [@hoeting_bayesian_1999]. The major difficulties for BMA are (1) how to sample models to test, and (2) how to calculate the posterior model probability given the data (or $p(M|D)$ where $M$ is the model, and $D$ is the data). For model selection, we use a Markov Chain Monte Carlo (MCMC) algorithm, similar in spirit and design to the reversible jump, MC3,and SVSS algorthims ```citations```, but specific to modelling piecewise constant breaks in a set of highly collinear continuous variables. We call it the Stochastic Window Sampler (SWS). To implement SWS, we use the following steps:1. Define a jumping distribution $g(.)$, so that $g(M \rightarrow M')$ is non-zero for all possible window constraints.2. Specify a starting model, $M$, and ellicit priors for models $M$ in $\mathscr{M}$.3. Given that the chain is in state $M$, draw $M'$, and accept it with probability$$min  \Bigg \{ 1, \frac{p(M'|D)}{p(M|D)} \Bigg \} $$otherwise, retain $M$.4. After a sufficient number of iterations, use all accepted models, $M$, and combine them using BMA to estimate APC effects.##Step One: Defining a Window Constraint Sampler and Jumping DistributionIn terms of window constraints, the target model, which is inestimable, is built by coding each unique value of age, period, and cohort, as a dummy variable series:$$E(Y) = \beta_{00} + \sum_{\lambda=2}^{\lambda} \beta_{1\lambda}A_{\lambda} +  \sum_{\rho=2}^{\rho} \beta_{2\rho}P_{\rho} +  \sum_{\kappa=2}^{\kappa} \beta_{3\kappa}C_{\kappa}$$In Equation 1, $\beta$ is an estimated effect. $\lambda$, $\rho$ and $\kappa$ index unique values for age, period, and cohort, and $A$, $P$, and $C$ stand for matricies of dummy variable series for age, period, and cohort. This model is unidentified, because, as with the continuous case, the dummy variables in any two of the matricies above fully condition the third matrix. In other words, the indicator variable in $C$ is a function of the indicators in $A$ and $P$ (in mathematical terms,the probability that any given cohort dummy variable is one or zero is exctly dependent on the values of A and P, so that $p(C_{\kappa}=0|A_{\lambda},P_{\kappa}$) is always exaclty zero, or exactly one, depending on the values of $A$ and $P$).We can break this dependencey, however, by transofrming $A$, $P$, and $C$---preferably without resorting to some prespecified arbitrary set of constraints [@gelman_bayesian_2014, at p. 366]. How do the window constraints break the linear dependency? By way of example, we can construct an age dummy variable series where $A$ is sliced into two groups based on some cut-point so that, for example, individuals who are older than 30 have a dummy variable of 1. This would identify a binary dummy variable for an older and a younger group. We can generalize this expression to an arbitrary vector of cut-points, $G$ with subscript $\gamma$ so that the window constraints of $A$ in Equation 1 are as follows:$$\mbox{for} \gamma < max(\gamma):A_{\lambda}  =\begin{cases}  1, & \mbox{if} & a > G_{\gamma} & \mbox{and} & a \leq G_{\gamma+1} \\  0, & \mbox{if} & a < G_{\gamma} & \mbox{or} & a > G_{\gamma+1}\end{cases}$$If the vector $G$ and $\gamma$ have the following properties: $max(G) = max(a)$, $min(G) < min(a)$, $G_2 \geq min(a)$ and $\gamma>2$, then permutatios of $G$ descirbe all posible sets of window restrictions for age. The first three requirement ensures that the dummy variable series $A$ is fully defined across the entire range of the continuous variable $a$. In particular, the first restriction ensures that the dummy variable series with the oldest ages in $G$ contains the maximum value of $a$. Similarly, the next two restrictins ensure that the smallest window constraint in $G$ contains the minimum value of $a$. The final constraint requires that $G$ have at least three elements. Three elements in $G$ defines a single dummy variable. Using the example above, if $a$ has a range of 5 to 50, then the dummy variable distinguishing older and younger respondents can be defined by $G \in \{4,30,50\}$. Generalizing cross all dimensions of APC by permuting three similar vectors (say $G^{(d)}$) will describe any model for any possible window constraints detailed in equation 1. Accordingly, all permutations of G, as defined above, constitute the model space of window constraints ($\mathscr{M}$). For any given set of APC variables, $\mathscr{M}$ is finite, but it can become large. For example, a set of continous ages, periods and cohorts weach with a range of 5 allows for 3,375 unique window models[^1]. Bump them all to  range of 6 and there are 38,304 unique window models. In any model space, only 1 model is inestimable because of perfect collinearity. Based on the statistical criticisms lobbied at APC models, critiques arge that this target model is (theoretially) the least biased, becasue it imposes the fewest assumptions on the models [@luo_block_2016; @bell_hierarchical_2017]. Although it is almost certainly the most parsimonious. The question is how to best use information from some subset of possible models in $\mathscr{M}$ to estimate unbiased APC effects of the target model. MCMC methods from Bayesian Model Averaging (BMA) provides a straightforward way to combine models. Generally speaking, these methods search across some predefined neighborhood of models, and then use a weighted average of the posterior esitmates for inference.[^1]: A continuous variable of 5 integers can be sliced into continous window constraints in 15 ways (where | indicates a window break for dummy variables):      2 windows, 4 combinations: 1|2345, 12|345, 123|45, 1234|5            3 windows, 6 combinations: 1|2|345, 1|23|45, 1|234|5, 12|3|45, 12|34|5, 123|4|5            4 windows, 4 combinations: 1|2|3|45, 1|2|34|5, 1|23|4|5, 12|3|4|5            5 windows, 1 combination: 1|2|3|4|5            Since the hypothetical assumes 3 variables (A,P,and C) of 15 combinations each, total combinations are $15^3=3,375$. Similar calculations over an integer of 6 leads to 34 window combinations over each dimension for a total of 39,304 possible models.      Because $G^{(d)}$ as defined in equation __ describes all possible window models for equation 1, any process that samples from $G$ by definition samples from all potential models of window breaks. Accordingly, sampling window breaks is simply a matter of smapling appropriate values for $G$. In Bayesian statistics and data-driven inference like machine learning, thsese sorts of clustering problems are often solved using a Dirchelet distribution. *Sampling the Window Groups (G) using the Dirchelet Distribution.* There are two basic features of vector $G$. First, is the number of window breaks, or the rank of $G$, and second is the location of the window breaks, or the probablility that particular values of the continuous variables will occur in $G$.  We use the Dirichelet distributon to sample both the location and number of window breaks. The Dirichelet is well-suited to this task, and is commonly used in classification tasks ```citation```. In additon, Taddy et al. [-@taddy_bayesian_2015] show that the Dirichelet is a natural prior distribution for variable selection and value-splitting in classification and regression trees (CART) algortithms, common for machine learning. Our approach is a similar, as developing $G$ is fundamentally a classificaiton task which aggregates similar ages, periods, and cohorts. Finally, a classical description of the Dirichelet distribution is one of "string-cutting." This is preciesely the task in sampling $G$---to cut a continuous string of variables into some subset of shorter lengths, where the effects are equal. To implement our sampling scheme, we use a set of auxiliary variables for each dimension ($d$) of APC. These are the intensity, or weight values for the Dirchelet distribution ($\alpha$), which are numbers between 0 and infinity. Various combinations of $\alpha$ sample from the simplex (represented as $B$ below) for each unique value of A, P, or C. The simplex is a vector of numbers between 0 and 1 whcih sum to 1. $G^{(d)}$ is the rounded, cumulative sum of the product $B$ times $w$. More formally, we consruct $G$ as follows.$$G^{(d)}_b = W^{(d)}_i, where  W^{(d)}_i = \Bigg \lfloor max(\tau^{(d)}) \sum_{i=1}^{\tau_d} B^{(d)}_i \Bigg \rceil,	  \mbox{iff} G^{(d)}_b > G^{(d)}_{b-1} \mbox{and} b \subseteq \tau_d$$$$B^{(d)} \sim Dir(\alpha_{\tau_d}) $$Where $\tau_{d}$ is the index number for the APC effects (the $\lambda$, $\rho$, or $\kappa$ of equaiton 1) $T$ is the continuous vector of values (the $A$,$P$, or $C$ of equation 1). We take the rounded value of the product of the cumulative sum of $B$ and the maximum index value of A, P, or C, which provides a stirng of integers from 1 to the maximum index values. These integers identify continuous groupings of windows. Back to the binary example above, the floor-rounded cumulative sum, $W$ would be a vector of ```26 ones, followed by 20 twoes --- this is wrong; needs edited; as does the next two sentences```. $G$ is simply the unique elements of $W$, begins before the minumum value, and ends after the maximum value. In other words $G$, sample d in this manner, describes a unique window model of the form laid out in Equation __. Moving from one realization of $G$ to another, is a matter of changing the $\alpha$ from one set of models to the next.As commonly used in MCMC methods, we use a multivariate normal distribution with a specified variance to update $\alpha$ from one model ($M$) to the next ($M'$) ```cite scott's book```. This produces a random walk on the auxiliary parameter $\alpha$ which sequentially updates the probability of window breaks, by adjusting the probability of a window break at each value of APC. Based on the acceptance probability described below, the chain will converge to better fitting sets of window models as the chain approaches infinity.##Step 2: Specify a Starting Model $M$ This constuction makes the ellicitation of more and less informative priors straightforward. In particular, the $\alpha$ identifies the probability that a break occurs between two contiguous groupings of age, period or cohort. An larger $\alpha$ value represents a more diffuse simplex [@gelman_bayesian_2014, p. 69]. Smaller values indicate less weight, *i.e.* it is less likely to be a window break, and larger values indicate more weight, or it is more likely to be a window break. The meaning of $\alpha$ parameter in the Dirichelet distriution for equations __ to __ is pictured in Figure 1 below.Each panel of Figure 1 depicts 1,000 random draws from Equations __ and __ across a continuous variable of 8 dimensions. In the top panel, each $\alpha$ is equal to 1. The bar graph reports the proportion of times that each dimension represents a new windows break. The first dimension is equal to 100% (as required to span the entire range, the first dummy variable must begin with the minimum value). Beyond that, the rest begin a window break nearly all of the time, with an average number of breaks equal to 7.2. By contrast, the bottom panel has a non-uniform array of $\alpha$ values spanning 0.3 to 17.8. In response, the probability of window breaks is different between each value, with the highest probablities corresponding to the highest values in $\alpha$, and the lowest values corresponding to the lowest values of $\alpha$. The average number of window breaks in for this distribution is about one-third less than the top panel at 4.4. \[Figure 1 About Here\]Figure 1 illustrates that the higher the value $\alpha$, the higher the probability that the corresponding observed value will be a  window break. Accordingly, the Dirichelet also makes it easy to specify more and less informative priors. A researcher can ellicit a more informative prior by placing larger values of $\alpha$ on particular values of a dimension $d$ to make it more probable that this will constitute a window break. The final requirement of this step is to choose a starting model $M$. We choose $M$ such that it starts close to the target model, *i.e.*, there are a large number of window breaks. To accomplish this, we set the initial value of each $\alpha$ to the dimensionality of the A, P, or C. As with the example outlined in Figure 1, the mean number of window breaks between the maximum value of each A, P, or C. This is consistent with the interpretation of $\alphas$ as "prior sample size" ```cite gelman```. This prior sample size is approximately the size of the window breaks less one (as the mean value in the xample above). Alternative starting values are possible, as are different starting values for multiple MCMC chains.##Step 3: Accept or Reject Proposed Models in a Markov ChainHaving identified the sampling procedure, the jumping kernel, and the starting vaues, the next step is sample proposal models and accept or reject them. As outlined above, each chain takes the current model, $M$, and samples $M'$ by permuting the vector of $\alphas$ from a normal distribution. These $\alpha'$ values are used to draw a model of window constraints $M'$. Whether to accpet $M'$ in favor of $M$ becomes a problem of model comparison. A standard procedure for model comparison of non-nested models is the Bayes Factor, which is the marginal probability of model $M$ divided by the marginal probability of model $M'$. We adopt the jumping kernel described in ```citation```, which accepts model $M'$ with a probability $R$ as follows.```you need to read the MC3 models to justify this...```$$R = min  \Bigg \{ 1, \frac{p(M'|D)}{p(M|D)} \Bigg \} $$The chain $M'$ is accepted if $R$ is greater than a value drawn from a uniform distribution between 0 and 1 ($U \sim (0,1)$). The Bayes Factor above can be difficult to calculate, depending on the model and the priors. There are, however, a number of possibilities, including use of the BIC approximation to the Bayes Factor```citaiton```.##Step 4: Ensemble Accepted Models in the MCMC Chain Finally, after a specified number of iterations across the Markov Chain, any desired estimate, $\Delta$ can be calculated using the weighted average of the effects times the posterior model probabilities.$$E(\Delta|D) = \sum_{k=0}^{K} \hat{\Delta} \pi(M_k|D)$$As before, the probabilites can be difficult to calculate. However, decades of work have developed these estimates, including an extensive classification laid out by Raftery [-@raftery_bayesian_1995] more than 20 years ago in *Sociological Methodology*. #A Simulationa linear model that reduces to the OLS estimation of .#An Empirical Example Using the GSS#Discussion#Conclusion**References**