---author:	- name: Bryce Bartlett	- name: Stephen Vaiseytitle: Ensembling Age-Period-Cohort Estimates by Averaging Multiple Window Constraint Models using Markov Chain Monte Carlo Methods bibliography: citations/apc.bibcsl: citations/asa-mod.csl---```rounding makes equal probability --- NOT floor```#AbstractKey words: Computational Sociology, Methods, Life Course, Bayesian, Culture#Introduction#BackgroundBoth classic and modern studies have proposed different effects across three social dimensions of time: age, period, and cohort [@ryder_cohort_1965]. These dimensions of time operate on individuals in different ways as they flow through social space. Age effects are driven either by the common experiences associated with the biological process of aging [@jackson_biological_2003], or persistent age-structuring institutions, like high school or retirement [@mortimer_government_2003; @waite_constrained_2014].  Period effects are responses of everyone to contemporaneous social experiences, like recessions or wars [@lam_is_2014]. Finally, cohort effects are socialization effects. They are broad-based historical events that stick to the populations experiencing them, even after the event has long since ended [@vaisey_cultural_2016]. In contemporary research, cohorts are virtually always defined in terms of birth year.These three dimensions of time sit at the center of some recurrent debates. For example, cultural sociologists are divided over whether the main driver of culture is a period process (cultural fragmentation) or a cohort process (acquired dispositions) [@vaisey_cultural_2016]. Clinical reasearchers and biodemographers are looking for biological age---indicators of biological deterioration---but argue about confounding from cohort changes [@jackson_biological_2003]. And yet others are concerned about separating long-term and short-term impacts of important events, like the Great Recession [@burgard_effects_2015].Unfortunately, it is not trivial to statistically estimate the unique effects of age, period, and cohort which would allow analysts to evaluate evidence that may settle these questions. The fundamental problem is one of statistical identification: these three dimensions are linearly dependent. Two of the dimensions define the third. If a researcher knows an individual's age, and the year of the survey, cohort is also defined. Because these variables are exactly colinear, they are not estimable using classic statistical techniques. A number of solutions have been proposed to address this conundrum, these include traditional methods like block/window constraints [@glenn_strategies_2005], and new methods, including statistical transformation (the "intrinsic estimator"), and random effects models [@yang_age-period-cohort_2013]. The past several years have seen a resurgence in debates surrounding on methodology [@luo_block_2016; @luo_assessing_2013; @bell_hierarchical_2017]. Perhaps the most common argument is that the "constraints", *i.e.* assumptions used to break the APC identity, produce unknown biases into the models. These particular complaints represent the extreme case of multicollinearity, a well-known, though difficult problem in statistical theory [@wooldridge_introductory_2009, at p. 95-98], with numerous cautions in applied texts that separating highly colinear effects is difficult [@camm_essentials_2016, at p. 328]. Viewing the APC identity problem as an issue of multicolinnearity in a Bayesian perspective provides a simple way to understand and adcress the problem. Ultimately, multicollinearity significantly reduce the ability of the data to supply information to produce the estimates, potentially introducing sensitivity to the prior [@gelman_bayesian_2014, at p. 305-306], including assumtpions of linearity. To put it another way, the critique of APC models in general, and window constraints in particular [@luo_block_2016], is that different sets of assumptions lead to different (and inconsistent) results. But, what if instead of focusing on finding a *best* fitting model, we use a simple nonlinear approach (block or window modelling), and hundreds of assumptions? Ensembling is the combination of estimates from different models subject to some weighting scheme. Bayesian Model Averaging (BMA) allows us to do just that with the APC problem: estimate hundreds of models with differeing assumptions, and then combine them, weighted by a measure of the model's posterior probability.#Bayesian Model AveragingThe theoretical backdrop of BMA applies to this curcumstance quite well. In principal, there is no one true (or best) model; instead estimates are conditional on models from the modeling space ($\mathscr{M}$), and have  a posterior distribution, which is calculated as a weighted average of all models [@raftery_bayesian_1995, p. 144-145].  It has been applied in diverse areas from weather forecasting to biology to social science [@fragoso_bayesian_2015] . BMA operates under the simple fact that any particular estimate, including effect size and significance, have a posterior probability distribution which is calculated as "an average of the posterior distributions under each of the models considered, weighted by their posterior model probability." [@hoeting_bayesian_1999]. The major difficulties for BMA are (1) how to sample models to test, and (2) how to calculate the posterior model probability given the data (or $p(M|D)$ where $M$ is the model, and $D$ is the data). For model selection, we use a Markov Chain Monte Carlo algorithm, similar in spirit and design to the reversible jump, MC3,and SVSS algorthims ```citations```, but specific to modelling piecewise constant breaks in a set of highly collinear continuous variables. We call it the Stochastic Window Sampler (SWS). To implement SWS, we use the following steps:1. Define a jumping distribution $g(.)$, so that $g(M \rightarrow M')$ is non-zero for all possible window constraints.2. Specify a starting model, $M$, and ellicit priors for models $M$ in $\mathscr{M}$.3. Given that the chain is in state $M$, draw $M'$, and accept it with probability$$min  \Bigg \{ 1, \frac{p(M'|D)}{p(M|D)} \Bigg \} $$otherwise, retain $M$.4. After a sufficient number of iterations, use all accepted models, $M$, and combine them using BMA to estimate APC effects.##Step One: Defining a Window Constraint SamplerIn terms of window constraints, the target model, which is inestimable, is built by estimating each unique value of age, period, and cohort, as a dummy variable series:$$E(Y) = \beta_{00} + \sum_{\lambda=2}^{\lambda} \beta_{1\lambda}A_{\lambda} +  \sum_{\rho=2}^{\rho} \beta_{2\rho}P_{\rho} +  \sum_{\kappa=2}^{\kappa} \beta_{3\kappa}C_{\kappa}$$$\beta$ is an estimated effect. $\lambda$, $\rho$ and $\kappa$ index unique values for age, period, and cohort, and $A$, $P$, and $C$ stand for matricies of dummy variable series for age, period, and cohort. This model is unidentified, because, as with the continuous case, the dummy variables in any two of the matricies above fully condition the third matrix. In other words, the indicator variable in $C$ is a function of the indicators in $A$ and $P$ (in mathematical terms,the probability that any given cohort dummy variable is one or zero is exctly dependent on the values of A and P, so that $p(C_{\kappa}=0|A_{\lambda},P_{\kappa}$) is always exaclty zero, or exactly one, depending on the values of $A$ and $P$).We can break this dependencey, however, by transofrming $A$, $P$, and $C$---preferably without resulting to some prespecified arbitrary set of constraints [@gelman_bayesian_2014, at p. 366]. How do the window constraints break the linear dependency? By way of example, we can construct an age dummy variable series where $A$ is sliced into two groups based on some cut-point so that, for example, individuals who are older than 30 have a dummy variable of 1. This would identify a binary dummy variable for an older and a younger group. We can generalize this expression to an arbitrary vector of cut-points, $G$ with subscript $\gamma$ so that the window constraints of $A$ in equation __ are as follows:$$\mbox{for} \gamma < max(\gamma):A_{\lambda}  =\begin{cases}  1, & \mbox{if} & a > G_{\gamma} & \mbox{and} & a \leq G_{\gamma+1} \\  0, & \mbox{if} & a < G_{\gamma} & \mbox{or} & a > G_{\gamma+1}\end{cases}$$If the vector $G$ and $\gamma$ have the following properties: $max(G) = max(a)$, $min(G) < min(a)$, $G_2 \geq min(a)$ and $\gamma>2$, then $G$ can describe any posible sets of window restrictions for age. The first three requirement ensures that the dummy variable series $A$ is fully defined across the entire range of the continuous variable $a$. In particular, the first restriction ensures that the dummy variable series with the oldest ages in $G$ contains the maximum value of $a$. Similarly, the next two restrictins ensure that the smallest window constraint in $G$ contains the minimum value of $a$. The final constraint requires that $G$ have at least three elements. Three elements in $G$ defines a dummy variable. Using the example above, if $a$ has a range of 5 to 50, then the dummy variable distinguishing older and younger respondents can be defined by $G \in \{4,30,50\}$. Generalizing cross all dimensions of APC, permuting three similar vectors (say $G^{(d)}$) will describe any model for any possible window constraints detailed in equation 1. Accordingly, all permutations of G, as defined above, constitute the model space of window constraints ($\mathscr{M}$). For any given set of APC variables, $\mathscr{M}$ is finite, but it can become large. For example, a set of continous ages, periods and cohorts weach with a range of 5 allows for 3,375 unique window models[^1]. Bump them all to  range of 6 and there are 38,304 unique window models. In any model space, only 1 model is inestimable because of perfect collinearity. Based on the statistical criticisms lobbied at APC models, critiques arge that this target model is (theoretially) the least biased, becasue it imposes the fewest assumptions on the models [@luo_block_2016; @bell_hierarchical_2017]. Although it is almost certainly the most parsimonious. The question is how to best use information from some subset of possible models in $\mathscr{M}$ to estimate unbiased APC effects of the target model. Bayesian Model Averaging (BMA) provides a straightforward way to combine models. ```need transition```[^1]: A continuous variable of 5 integers can be sliced into continous window constraints in 15 ways (where | indicates a window break for dummy variables):      2 windows, 4 combinations: 1|2345, 12|345, 123|45, 1234|5            3 windows, 6 combinations: 1|2|345, 1|23|45, 1|234|5, 12|3|45, 12|34|5, 123|4|5            4 windows, 4 combinations: 1|2|3|45, 1|2|34|5, 1|23|4|5, 12|3|4|5            5 windows, 1 combination: 1|2|3|4|5            Since the hypothetical assumes 3 variables (A,P,and C) of 15 combinations each, total combinations are $15^3=3,375$. Similar calculations over an integer of 6 leads to 34 window combinations over each dimension for a total of 39,304 possible models.      There are two features of $G$ that make an MCMC method an attractive way to sample window models. First, all of the models can be described by a set of three vectors, $G$. Sampling window breaks is simply a matter of smapling appropriate values for $G$. In Bayesian statistics and data-driven inference like machine learning, thsese sorts of clustering problems are often solved using a Dirchelet distribution. *Sampling the Window Groups (G).* There are two basic features of vector $G$. First, is the number of window breaks, or the rank of $G$, and second is the location of the window breaks, or the probablility that particular values of the continuous variables will occur in $G$.  We use the Dirichelet distributon to sample the location of window breaks. The Dirichelet is well-suited to this task, and is commonly used in classification tasks ```citation```. In additon, Taddy et al. [-@taddy_bayesian_2015] show that the Dirichelet is a natural prior distribution for variable selection and value-splitting in classification and regression trees (CART) algortithms, common for machine learning. Our approach is a similar, as developing $G$ is fundamentally a classificaiton task which aggregates similar ages, periods, and cohorts. Finally, a classical description of the Dirichelet distribution is one of "string-cutting." This is preciesely the task in sampling $G$---to cut a continuous string of variables into some subset of shorter lengths, where the estimates are assumed to be equal. To implement our sampling scheme, we a set of auxiliary variables for each dimension ($d$) of APC. These are the intensity, or weight values for the Dirchelet distribution, which .The fist is a Simplex ($B$) for each unique value of A, P, or C. We draw a unique simplex for each dimension $d$, with the same length of unique elements in $d$ ($B^{(d)}$) using the Diricelet distribution. The second auxiliary variable is a scalar for each dimension, $w^d$. We use a uniform distribution to sample $w$. $G^{(d)}$ is simply the product of $B$ times $w$. More formally, we consruct $G$ as follows.$$G^{(d)}_b = W^{(d)}_i, where  W^{(d)}_i = \Bigg \lfloor max(\tau^{(d)}) \sum_{i=1}^{\tau_d} B^{(d)}_i \Bigg \rceil,	  \mbox{iff} G^{(d)}_b > G^{(d)}_{b-1} \mbox{and} b \subseteq \tau_d$$$$B^{(d)} \sim Dir(\alpha_{\tau_d}) $$```this is a confusing paragraph```Where $\tau_{d}$ is the index number for the APC effects (the $\lambda$, $\rho$, or $\kappa$ of equaiton 1) $T$ is the continuous vector of values (the $A$,$P$, or $C$ of equation 1). We take the floor rounded value of the product of the cumulative sum of $B$ and $w$, which provides a stirng of integers from 1 to $w$. These integers identify continuous groupings of windows. Back to the binary example above, the floor-rounded cumulative sum, $W$ would be a vector of 26 ones, followed by 20 twoes. The starting value $W_1$ is the minimum value of the $a$, $p$, or $c$, but each subsequent value of the vecotr $W$ is the value of age where the vector of indexes in $w$ times $B$ change. $G$ is simply defined as the unique elements of $W$, begins before the minumum value, and ends after the maximum value. In other words $G$, sample d in this manner, describes a unique window model of the form laid out in Equation __.There are a number of possibilites for the jumping distribution of $g(.)$. Specifically, we use a multivariate normal distribution with a specified variance to update $\alpha$ from one set of models to the next. As such, this produces a random walk on the auxiliary parameter $\alpha$ which sequentially updates the probability of window breaks, by adjusting the probability of a window break at each value of APC. Based on the acceptance probability described below, the chain will converge to better fitting sets of window models as the chain approaches infinity.##Step 2: Specify a Starting Model $M$ This constuction makes the ellicitation of more and less informative priors straightforward. In particular, the $\alpha$ identifies the probability that a break occurs between two contiguous groupings of age, period or cohort. An $\alpha$ equal to 1 is a uniform (or noninformative) distribution across the simplex [@gelman_bayesian_2014, p. 69]. Values less than 1 indicate less weight, *i.e.* it is less likely to be a window break; wherease, values more than 1 indicate more weight, or it is more likely to be a window break. The meaning of $\alpha$ parameter in the Dirichelet distriution for equations __ to __ is pictured in Figure 1 below. Each panel depicts 1,000 random draws from the equations above across 10 dimensions, where $w=9$. In the bottom panel, each $\alpha$ is equal to 1. The bar graph reports the proportion of times that each dimension represents a new windows break. The first dimension is equal to 100% (as required to span the entire range, the first dummy variable must begin with the minimum value). Beyond that, the rest begin a window break about 63% of the time.Figure 1 displays the mean probability that a window break begins at each dimension. For the bottom panel, the mean probability is 10% for all of the windows. In the bottom panel, $\alpha = [0.2,0.2,0.2,1,1,1,1,2,2,2]. The mean probability of a window break is low for the first three (2%), roughly even for the middle 4 (9%), and high for the final 3 (19%). \[Figure 1 About Here\]The higher the value on the simplex, the higher the probability that the value will be a window break. Accordingly, the Dirichelet also makes it easy to specify more and less informative priors. A dirichelet with values of 1 for $\alpha$ constitute equal probability of a window break. A researcher can ellicit a more informative prior by placing larger values of $\alpha$ on particular values of a dimension $d$ to make it more probable that this will constitute a window break. Multiplying the vector of weights ($B$) by a scalar $w$ provides a set of numbers which sum to the scalar value $w^{(d)}$. ##Step 3: Accept or Reject Proposed Models##Step 4: Ensemble Accepted Models Using BMA#A Simulation#An Empirical Example Using the GSS#Discussion#Conclusion