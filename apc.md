---author:	- name: Bryce Bartlett	- name: Stephen Vaiseytitle: Ensembling Age-Period-Cohort Estimates by Averaging Multiple Window Constraint Models using Markov Chain Monte Carlo Stochastic Window Sampler bibliography: citations/apc.bibcsl: citations/asa-mod.csl---#AbstractStochastic Window Sampler (SWS). ```incorporate this```Key words: Computational Sociology, Methods, Life Course, Bayesian, Culture#Introduction#BackgroundBoth classic and modern studies have proposed different effects across three social dimensions of time: age, period, and cohort [@ryder_cohort_1965]. These dimensions of time operate on individuals in different ways as they flow through social space. Age effects are driven either by the common experiences associated with the biological process of aging [@jackson_biological_2003], or persistent age-structuring institutions, like high school or retirement [@mortimer_government_2003; @waite_constrained_2014].  Period effects are responses of everyone to contemporaneous social experiences, like recessions or wars [@lam_is_2014]. Finally, cohort effects are socialization effects. They are broad-based historical events that stick to the populations experiencing them, even after the event has long since ended [@vaisey_cultural_2016]. In contemporary research, cohorts are virtually always defined in terms of birth year.These three dimensions of time sit at the center of some recurrent debates. For example, cultural sociologists are divided over whether the main driver of culture is a period process (cultural fragmentation) or a cohort process (acquired dispositions) [@vaisey_cultural_2016]. Clinical reasearchers and biodemographers are looking for biological age---indicators of biological deterioration---but argue about confounding from cohort changes [@jackson_biological_2003]. And yet others are concerned about separating long-term and short-term impacts of important events, like the Great Recession [@burgard_effects_2015].Unfortunately, it is not trivial to statistically estimate the unique effects of age, period, and cohort which would allow analysts to evaluate evidence that may settle these questions. The fundamental problem is identification: these three dimensions are linearly dependent. Two of the dimensions define the third. If a researcher knows an individual's age, and the year of the survey, cohort is also defined. Because these variables are exactly colinear, they are not estimable using classic statistical techniques. A number of solutions have been proposed to address this conundrum, these include traditional methods like block/window constraints [@glenn_strategies_2005], and new methods, including statistical transformation (the "intrinsic estimator"), and random effects models [@yang_age-period-cohort_2013]. The past several years have seen a resurgence in debates surrounding on methodology [@luo_block_2016; @luo_assessing_2013; @bell_hierarchical_2017]. Perhaps the most common argument is that the "constraints", *i.e.* assumptions used to break the APC identity, produce unknown biases into the models. These particular complaints represent the extreme case of multicollinearity, a well-known, though difficult problem in statistical theory [@wooldridge_introductory_2009, at p. 95-98], with numerous cautions in applied texts that separating highly colinear effects is difficult [@camm_essentials_2016, at p. 328]. Viewing the APC identity problem as an issue of multicolinnearity in a Bayesian perspective provides a simple way to understand and adcress the problem. Ultimately, multicollinearity significantly reduce the ability of the data to supply information to produce the estimates, potentially introducing sensitivity to the prior [@gelman_bayesian_2014, at p. 305-306], including assumtpions of linearity. To put it another way, the critique of APC models in general, and window constraints in particular [@luo_block_2016], is that different sets of assumptions lead to different (and inconsistent) results. But, what if instead of focusing on finding a *best* fitting model, we use a simple nonlinear approach (block or window modelling), and hundreds of assumptions? Ensembling is the combination of estimates from different models subject to some weighting scheme. Bayesian Model Averaging (BMA) allows us to do just that with the APC problem: estimate hundreds---perhaps even thousands---of models with differeing assumptions, and then combine them, weighted by a measure of the model's posterior probability.#Bayesian Model AveragingThe theoretical backdrop of BMA applies to this curcumstance quite well. In principal, there is no one true (or best) model; instead estimates are conditional on models from the modeling space ($\mathscr{M}$), and have  a posterior distribution, which is calculated as a weighted average of all models [@raftery_bayesian_1995, p. 144-145].  It has been applied in diverse areas from weather forecasting to biology to social science [@fragoso_bayesian_2015] . BMA operates under the simple fact that any particular estimate, including effect size and significance, have a posterior probability distribution which is calculated as "an average of the posterior distributions under each of the models considered, weighted by their posterior model probability." [@hoeting_bayesian_1999]. The major difficulties for BMA are (1) how to sample models to test, and (2) how to calculate the posterior model probability given the data (or $p(M|D)$ where $M$ is the model, and $D$ is the data). For model selection, we use a Markov Chain Monte Carlo (MCMC) algorithm, similar in spirit and design to the reversible jump, MC3,and SVSS algorthims ```citations```, but specific to modelling piecewise constant breaks in a set of highly collinear continuous variables. We call it the Stochastic Window Sampler (SWS). To implement SWS, we use the following steps:1. Define a jumping distribution $g(.)$, so that $g(M \rightarrow M')$ is non-zero for all possible window constraints.2. Specify a starting model, $M$, and ellicit priors for models $M$ in $\mathscr{M}$.3. Given that the chain is in state $M$, draw $M'$, and accept it with probability$$min  \Bigg \{ 1, \frac{p(M'|D)}{p(M|D)} \Bigg \} $$otherwise, retain $M$.4. After a sufficient number of iterations, use all accepted models, $M$, and combine them using BMA to estimate APC effects.##Step One: Defining a Window Constraint Sampler and Jumping DistributionIn terms of window constraints, the target model, which is inestimable, is built by coding each unique value of age, period, and cohort, as a dummy variable series:$$E(Y) = \beta_{00} + \sum_{\lambda=2}^{\lambda} \beta_{1\lambda}A_{\lambda} +  \sum_{\rho=2}^{\rho} \beta_{2\rho}P_{\rho} +  \sum_{\kappa=2}^{\kappa} \beta_{3\kappa}C_{\kappa} \label{eq:1}$$In Equation 1, $\beta$ is an estimated effect. $\lambda$, $\rho$ and $\kappa$ index unique values for age, period, and cohort, and $A$, $P$, and $C$ stand for matricies of dummy variable series for age, period, and cohort. This model is unidentified, because, as with the continuous case, the dummy variables in any two of the matricies above fully condition the third matrix. In other words, the indicator variable in $C$ is a function of the indicators in $A$ and $P$ (in mathematical terms,the probability that any given cohort dummy variable is one or zero is exctly dependent on the values of A and P, so that $p(C_{\kappa}=0|A_{\lambda},P_{\kappa}$) is always exaclty zero, or exactly one, depending on the values of $A$ and $P$).We can break this dependencey, however, by transofrming $A$, $P$, and $C$---preferably without resorting to some prespecified arbitrary set of constraints [@gelman_bayesian_2014, at p. 366]. How do the window constraints break the linear dependency? By way of example, we can construct an age dummy variable series where $A$ is sliced into two groups based on some cut-point so that, for example, individuals who are older than 30 have a dummy variable of 1. This would identify a binary dummy variable for an older and a younger group. We can generalize this expression to an arbitrary vector of cut-points, $G$ with subscript $\gamma$ so that the window constraints of $A$ in Equation 1 are as follows:$$\mbox{for} \gamma < max(\gamma):A_{\lambda}  =\begin{cases}  1, & \mbox{if} & a > G_{\gamma} & \mbox{and} & a \leq G_{\gamma+1} \\  0, & \mbox{if} & a < G_{\gamma} & \mbox{or} & a > G_{\gamma+1}\end{cases}$$If the vector $G$ and $\gamma$ have the following properties: $max(G) = max(a)$, $min(G) < min(a)$, $G_2 \geq min(a)$ and $\gamma>2$, then permutatios of $G$ descirbe all posible sets of window restrictions for age. The first three requirement ensures that the dummy variable series $A$ is fully defined across the entire range of the continuous variable $a$. In particular, the first restriction ensures that the dummy variable series with the oldest ages in $G$ contains the maximum value of $a$. Similarly, the next two restrictins ensure that the smallest window constraint in $G$ contains the minimum value of $a$. The final constraint requires that $G$ have at least three elements. Three elements in $G$ defines a single dummy variable. Using the example above, if $a$ has a range of 5 to 50, then the dummy variable distinguishing older and younger respondents can be defined by $G \in \{4,30,50\}$. Generalizing cross all dimensions of APC by permuting three similar vectors (say $G^{(d)}$) will describe any model for any possible window constraints detailed in equation 1. Accordingly, all permutations of G, as defined above, constitute the model space of window constraints ($\mathscr{M}$). For any given set of APC variables, $\mathscr{M}$ is finite, but it can become large. For example, a set of continous ages, periods and cohorts weach with a range of 5 allows for 3,375 unique window models[^1]. Bump them all to  range of 6 and there are 38,304 unique window models. In any model space, only 1 model is inestimable because of perfect collinearity. Based on the statistical criticisms lobbied at APC models, critiques arge that this target model is (theoretially) the least biased, becasue it imposes the fewest assumptions on the models [@luo_block_2016; @bell_hierarchical_2017]. Although it is almost certainly the most parsimonious. The question is how to best use information from some subset of possible models in $\mathscr{M}$ to estimate unbiased APC effects of the target model. MCMC methods from Bayesian Model Averaging (BMA) provides a straightforward way to combine models. Generally speaking, these methods search across some predefined neighborhood of models, and then use a weighted average of the posterior esitmates for inference.[^1]: A continuous variable of 5 integers can be sliced into continous window constraints in 15 ways (where | indicates a window break for dummy variables):      2 windows, 4 combinations: 1|2345, 12|345, 123|45, 1234|5            3 windows, 6 combinations: 1|2|345, 1|23|45, 1|234|5, 12|3|45, 12|34|5, 123|4|5            4 windows, 4 combinations: 1|2|3|45, 1|2|34|5, 1|23|4|5, 12|3|4|5            5 windows, 1 combination: 1|2|3|4|5            Since the hypothetical assumes 3 variables (A,P,and C) of 15 combinations each, total combinations are $15^3=3,375$. Similar calculations over an integer of 6 leads to 34 window combinations over each dimension for a total of 39,304 possible models.      Because $G^{(d)}$ as defined in equation __ describes all possible window models for equation 1, any process that samples from $G$ by definition samples from all potential models of window breaks. Accordingly, sampling window breaks is simply a matter of smapling appropriate values for $G$. In Bayesian statistics and data-driven inference like machine learning, thsese sorts of clustering problems are often solved using a Dirchelet distribution. *Sampling the Window Groups (G) using the Dirchelet Distribution.* There are two basic features of vector $G$. First, is the number of window breaks, or the rank of $G$, and second is the location of the window breaks, or the probablility that particular values of the continuous variables will occur in $G$.  We use the Dirichelet distributon to sample both the location and number of window breaks. The Dirichelet is well-suited to this task, and is commonly used in classification tasks ```citation```. In additon, Taddy et al. [-@taddy_bayesian_2015] show that the Dirichelet is a natural prior distribution for variable selection and value-splitting in classification and regression trees (CART) algortithms, common for machine learning. Our approach is a similar, as developing $G$ is fundamentally a classificaiton task which aggregates similar ages, periods, and cohorts. Finally, a classical description of the Dirichelet distribution is one of "string-cutting." This is preciesely the task in sampling $G$---to cut a continuous string of variables into some subset of shorter lengths, where the effects are equal. To implement our sampling scheme, we use a set of auxiliary variables for each dimension ($d$) of APC. These are the intensity, or weight values for the Dirchelet distribution ($\alpha$), which are numbers between 0 and infinity. Various combinations of $\alpha$ sample from the simplex (represented as $B$ below) for each unique value of A, P, or C. The simplex is a vector of numbers between 0 and 1 which sum to 1. $G^{(d)}$ is the rounded, cumulative sum of the product $B$ times the maximum possible number of window breaks (*i.e.* the number of unique values). More formally, we consruct $G$ in three steps as follows:First, we draw a random simplex from a Dircelet distribution across each dimension ($d$) of $A, P, and C$. The dimension of the simplex ($B$) is as large as the index ($\tau_{d}$) for each of the APC effects (the $\lambda$, $\rho$, or $\kappa$ of equation 1.$$B^{(d)} \sim Dir(\alpha_{\tau_d})$$Second, We take the cumulative sum of $B$ from step 1 and multiply it by the the maximum index value of A, P, or C ($\tau^{(d)}$). This provides a string of integers from 1 to $\tau^{(d)}$, covering the entire continuous range of each dimension. These integers identify a vector of window groupings by index numbers for each dimension ($W^{(d)}$. In terms of the dummy variable example from above, a continuous vector of ages between 5 and 50, recoded into a dummy variable with an younger (less than or equal to 30) and older group as wculd be described in terms of $W$ as a vector where 26 is repeated 26 times, (because $T_{26} = 31$), and where 45 is repeated 24 times (because $T_{45} = 50$.$$W^{(d)}_i = \Bigg \lfloor max(\tau^{(d)}) \sum_{i=1}^{\tau_d} B^{(d)}_i \Bigg \rceil, $$Third and finally, we use the vector of index numbers $W$ to construct $G$ as described in equation __. To do this, we begin the vector $G$ a value of 1 less than the minimum observed value (for the reasons described above). For each subsequent value, of $G$, we take use the value of $T$ described by each unique index number contained in $W$, as follows:  $$G^{(d)}_{b=1} = min(T^{(d)}-1, for b>1,$$$$G^{(d)}_b = T^{(d)}_{W^{(d)}_i}, \mbox{iff } G^{(d)}_b > G^{(d)}_{b-1},$$Following the steps above, $G$ is simply the unique elements of $W$, begins before the minumum value, and ends after the maximum value. In other words $G$, sample d in this manner, describes a unique window model of the form laid out in Equation __. Moving from one realization of $G$ to another, is a matter of changing the $\alpha$ from one set of models to the next. We visualize the impact $\alpha$ makes in determining the shape of $G$ below, where we discuss prior elicitation and the starting model, $M$.As commonly used in MCMC methods, we use a normal proposal distribution with a specified variance to update $\alpha$ from one model ($M$) to the next ($M'$) [@lynch_introduction_2007, p. 109]. This produces a random walk on the auxiliary parameter $\alpha$ which sequentially updates the probability of window breaks, by adjusting the probability of a window break at each value of APC. Based on the acceptance probability described below, the chain will converge to better fitting sets of window models with each additional draw of $G$.##Step 2: Specify a Starting Model $M$ Sampling window breaks as outlined in step 1 makes the ellicitation of more and less informative priors straightforward. In particular, the $\alpha$ identifies the probability that a break occurs between two contiguous groupings of age, period or cohort. An larger $\alpha$ value represents a more diffuse simplex [@gelman_bayesian_2014, p. 69]. Smaller values indicate less weight, *i.e.* it is less likely to be a window break, and larger values indicate more weight, or it is more likely to be a window break. The meaning of $\alpha$ parameter in the Dirichelet distriution for equations __ to __ is pictured in Figure 1 below.Each panel of Figure 1 depicts 1,000 random draws from Equations __ and __ across a continuous variable of 8 dimensions. In the top panel, each $\alpha$ is equal to 1. The bar graph reports the proportion of times that each dimension represents a new windows break. The first dimension is equal to 100% (as required to span the entire range, the first dummy variable must begin with the minimum value). Beyond that, the rest begin a window break nearly all of the time, with an average number of breaks equal to 7.2. By contrast, the bottom panel has a non-uniform array of $\alpha$ values spanning 0.3 to 17.8. In response, the probability of window breaks is different between each value, with the highest probablities corresponding to the highest values in $\alpha$, and the lowest values corresponding to the lowest values of $\alpha$. The average number of window breaks in for this distribution is about one-third less than the top panel at 4.4. \[Figure 1 About Here\]Figure 1 illustrates that the higher the value $\alpha$, the higher the probability that the corresponding observed value will be a  window break. Accordingly, the Dirichelet also makes it easy to specify more and less informative priors. A researcher can ellicit a more informative prior by placing larger values of $\alpha$ on particular values of a dimension $d$ to make it more probable that this will constitute a window break. The final requirement of this step is to choose a starting model $M$. We choose $M$ such that it starts close to the target model, *i.e.*, there are a large number of window breaks. To accomplish this, we set the initial value of each $\alpha$ to the dimensionality of the A, P, or C. As with the example outlined in Figure 1, the mean number of window breaks between the maximum value of each A, P, or C. This is consistent with the interpretation of $\alphas$ as "prior sample size" ```cite gelman```. This prior sample size is approximately the size of the window breaks less one (as the mean value in the xample above). Alternative starting values are possible, as are different starting values for multiple MCMC chains.##Step 3: Accept or Reject Proposed Models in a Markov ChainHaving identified the sampling procedure, the proposal density, and the starting vaues, the next step is sample proposal models and accept or reject them. As outlined above, each chain takes the current model, $M$, and samples $M'$ by permuting the vector of $\alphas$ from a normal distribution. These $\alpha'$ values are used to draw a model of window constraints $M'$. Whether to accpet $M'$ in favor of $M$ becomes a problem of model comparison. We adopt the jumping kernel described in Hoeting, et al. [-@hoeting_bayesian_1999], for the MC3 algortihm, which accepts model $M'$ with a probability $R$ based upon the minimum of 1 or the posterior model probabilities. We calculate the ratio of model probabiliteis using the BIC approximation to the Bayes factor [@raftery_bayesian_1995].$$\begin{aligned}R &= min  \Bigg \{ 1, \frac{p(M'|D)}{p(M|D)} \Bigg \} \\  &= min  \Bigg \{ 1, \frac{e^{BIC'-BIC}}{2} \} \end{aligned}$$The chain $M'$ is accepted over $M$ if $R$ is greater than a value drawn from a uniform distribution between 0 and 1 ($R > U \sim (0,1)$). Otherwise, $M$ is retained for the next draw in the chain. ##Step 4: Ensemble Accepted Models in the MCMC Chain Finally, after a specified number of iterations across the Markov Chain, any desired estimate, $\Delta$ can be calculated using the weighted average of the effects times the posterior model probabilities.$$E(\Delta|D) = \sum_{k=0}^{K} \hat{\Delta_k} \pi(M_k|D)$$As before, the probabilites can be difficult to calculate. However, decades of work have developed robust estimators, including an extensive classification laid out by Raftery [-@raftery_bayesian_1995] more than 20 years ago in *Sociological Methodology*. We use the BIC for the linear model outlined by Raftery in order to calculate the weighting scheme, so that ```add the equation here for pi```.#Exploratory SimulationsTo evaluate the effectiveness of our ensembling strategy, we simulate 6 different data sets with different quadratic age, period, and cohort effects. We draw three different sets of APC linear effects from a uniform distribution between -1 and 1; and three different sets of quadritic effects from a uniform distribution between -0.1 and 0.1. The smaller scale for the quadratic effects produces a more reasonable distribution of effects. We report the values used to simulate effects in the Appendix, S1. In order to compare cases with true nonlinear APC effects, we duplicate these three sets of effects, but delete a random A, P, or C.Given these effects, we simulate 1,000 observations as described in Luo and Hodges [-@luo_block_2016]. First, we generate 400 unique combinations of 20 ages (1 through 20) and 20 periods. We repeat this collection 25 times for a total of 1,000 total observations. We generate cohort as a linear combination of of age and period (period - age), and produce a squared term for each A, P, and C. We then square the observations and apply the effects in Appendix S1 to generate a predicted outcome value. To scale the $r^2$, we use a normally distributed error with mean 0 and standard deviation of 5.26. We store the average, predicted effects for comparison after we apply the BMA algorithm outlined above.To estimate effects on the simulated data, we apply the algorithm described above. We run 4 parallel MCMC chains of 250 iterations each for a total sample size of 1,000. In other words, we run separate 1,000 window models on the data. Nearly all of these models are different, as the window constraints are drawn given the $\alpha$ of the Dirchelet distribution (as depicted in Figure 1). Accordingly, even when Model $M'$ is rejected, a new model $M$ is drawn based on the prior $\alpha$ values for the Dirchele distribution. For each of the window breaks, we use a Gibbs sampler to draw 1,000 samples of each model, for a full posterior sample size of 1 million draws. For the Gibbs sampler, we use a uniform prior for the $\beta$ in equation 1, and an Inverse Gamma Distribution. These priors exactly coincide exactly with frequentist estimates and standard errors, but allow provide more information to evaluate model fit [@gelman_bayesian_2014]. We draw posterior samples using the composition method described in Lynch [-@lynch_introduction_2007, pp. 172-173].Because is is computationally intensive to sample 1,000,000 models, we use the simulation as an exploratory tool as opposed to a demonstrative tool. That is, we do not recalibrate the normal jumping distribution to change the accpetance rate, nor do we calibrate with other statistics. Instead, we simply run the models using identical, but arbitrarily determined characteristics, and compare the results to the true effects. We also use this to assess the effectiveness of unique tools for evaluating model fit from Bayesian estimators, such as test statistics from posterior predictive distributions.A unique tool for assessing model fit with Bayesian estimators is through posterior predictive data (PPD). To generate the PPD, we draw a sample of 1,000 sets of parameters from the entire posterior of 1 million samples using a weighted sampling scheme equal to the posterior model weight from the BMA analysis. We use these the PPD parameters to simulate predictive data of the observed (though simulated) outcome variable, $y$. The PPD distribution of $y$ is $\tilde{y}$, and we calculate the Bayesian p-value of the mean, or, the $\pi_{mean} = p(E(\tilde{y}) \geq E(y)$ [@lynch_introduction_2007, pp. 155-159]. Better-fitting models have Bayesian p-values near 0.5 (meaning that the mean of the replicated data is centered on the mean of the observed data) [@gelman_bayesian_2014, p. 151], so we also compute the absolute deviance of this p-value ($|\pi_{mean} -0.5|$). In other analyses, we used different test-statistics, including APC specific Bayesian p-values, and the range of age, period, and cohort specific means of $\tilde{y}$. The overall p-value was highly correlated with these other options and performed the best, so we retained this as the PPD test.We generate information for each set of models as reported in Table 1, and plot the best-fitted model, which is also the most concordant with true effects (Simulation 3) in Figure 2. Figure 2 shows extremeely close tracking of estimated values (dashed line) with actual values across all dimensions. In the supplement, Figure S-1 plots similar effects across all simulated effects.\[Figure 2 about here\]Table 1 reports the simulation number, and a number of variables to evaluate the model fit, MCMC convergence, and model concordence with the true effect. This table shows (1) the absolute deviance of the Bayesian p-value is an indicator of model fit. That is to say--generally--the smaller this deviance, the better the model matches the simulation values. (2) The absolute deviance value is sensitive to model $r^2$. (3) The SWS sampler is more effective at estimating nonlinear effects across all three dimensions.\[Table 1 about here\]The first two columns of Table 1 report the simulation number and whether the simulation includes all three effects. The thrid column colculates the proportion of effects (80 total) where the true effect is contained within the confidence intervals of the BMA-calculated main effects. A value of 1.0 would indicate exact concordance, and a value of 0 would indicate no overlap between estimates and true effects. A value of 0.05 would indicate 5% of the values fall outside of the 95% confidence intervals of the estimates. Note that this is an overly-conservative ratio, because it does not take into account expected uncertainty in estimating effects.For the best-fitting model (pictured in Figure 1), only 1 of the 80 effects fall outside of the predicted values. The other models do not fare as well, although the simulations with all three dimensions of time fare much better in general (simulations 1,3,and 5). This is not surprising, however, given that the construction of age-period-and cohort implies extremely high multicollinearity. Under these conditions, adding a variable which is highly correlated to a covariate is expected to produce collider bias ```I think this is correct -- add citation```. One particularly ill-fitting model (Simulation 2) misses almost 90% of the actual effects. However, the estimates are not as dire as the tabel itself indicates. The depictions of the model fits in Table S-1 show that the general trends are by and large reproduced. That is--appart from the null effect--trends generally track trends: when the actual effect is increasing, the estimated effect is increasing and when the actual effect is decreasing, the estimated effect is also decreasing. In addition, as suggested by Figure S-1, general groupings of ages, periods, and cohorts as similar would be generally accurate.The fourth column shows the absolute deviance of the Bayesian p-value of the mean as described above. Generally speaking, the smaller this value, the better the estimates track the true values. Simulation 3 has both the smallist discordance between estimates and true values, and the smallest absolute deviance. It is not always the case, however, and smaller model fits provide less predictable relatinshipos (as identified by the exception to this rule for Simulation 6, which has a relatively large deviance, lower discordance, but also low model r-squared).While the simulation results do not suggest that the SWS method is a silver-bullet for APC models, they do show significant promise in estimating APC effects. In particular, the SWS provides a way to test multiple model constraints, to aggregate APC classifications into groups, and to identify large-scale trends. The simulaiton results we provide are from a naive, exploratory perspective. These simulations show, however, that Bayesian estimates, such as the PPD correlate with concordance of true effects. We take this knowledge and apply it to an empirical example using trends of egalitarian attitudes as measured in the GSS.#An Empirical Example Using the GSSUsing the knowledge we #Discussion#Conclusion**References**