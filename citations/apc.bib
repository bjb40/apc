
@book{lynch_introduction_2007,
	location = {New York},
	title = {Introduction to Applied Bayesian Statistics and Estimation for Social Scientists},
	isbn = {978-0-387-71264-2},
	abstract = {This book outlines Bayesian statistical analysis in great detail, from the development of a model through the process of making statistical inference. The key feature of this book is that it covers models that are most commonly used in social science research - including the linear regression model, generalized linear models, hierarchical models, and multivariate regression models - and it thoroughly develops each real-data example in painstaking detail.},
	pagetotal = {359},
	publisher = {Springer},
	author = {Lynch, Scott M.},
	date = {2007-08-15}
}

@book{gelman_bayesian_2014,
	location = {Boca Raton},
	edition = {Third edition},
	title = {Bayesian data analysis},
	isbn = {978-1-4398-4095-5},
	series = {Chapman \& Hall/{CRC} texts in statistical science},
	abstract = {"Preface This book is intended to have three roles and to serve three associated audiences: an introductory text on Bayesian inference starting from first principles, a graduate text on effective current approaches to Bayesian modeling and computation in statistics and related fields, and a handbook of Bayesian methods in applied statistics for general users of and researchers in applied statistics. Although introductory in its early sections, the book is definitely not elementary in the sense of a first text in statistics. The mathematics used in our book is basic probability and statistics, elementary calculus, and linear algebra. A review of probability notation is given in Chapter 1 along with a more detailed list of topics assumed to have been studied. The practical orientation of the book means that the reader's previous experience in probability, statistics, and linear algebra should ideally have included strong computational components. To write an introductory text alone would leave many readers with only a taste of the conceptual elements but no guidance for venturing into genuine practical applications, beyond those where Bayesian methods agree essentially with standard non-Bayesian analyses. On the other hand, we feel it would be a mistake to present the advanced methods without first introducing the basic concepts from our data-analytic perspective. Furthermore, due to the nature of applied statistics, a text on current Bayesian methodology would be incomplete without a variety of worked examples drawn from real applications. To avoid cluttering the main narrative, there are bibliographic notes at the end of each chapter and references at the end of the book"--},
	pagetotal = {661},
	publisher = {{CRC} Press},
	author = {Gelman, Andrew},
	date = {2014},
	keywords = {Bayesian statistical decision theory, {MATHEMATICS} / Probability \& Statistics / General}
}

@article{burgard_effects_2015,
	title = {Effects of the Great Recession: Health and Well-Being},
	volume = {41},
	url = {http://dx.doi.org/10.1146/annurev-soc-073014-112204},
	doi = {10.1146/annurev-soc-073014-112204},
	shorttitle = {Effects of the Great Recession},
	abstract = {The existing evidence linking recessions to individual and population health presents a puzzle. Some studies show that people who experience the kinds of labor market, housing, and asset shocks that proliferate in recessions suffer negative health consequences, whereas other studies show that mortality rates fall when the economy worsens. This review synthesizes evidence from these distinct research traditions in light of emerging findings from the Great Recession of 2007–2009. It traces pathways by which macroeconomic changes “get under the skin” and generate contradictory aggregate- and individual-level consequences. Research on the longer-term health effects of recessions could be strengthened by integrating theoretical and analytical approaches from sociology. These include a multilevel perspective that considers how individuals cope with recessions as members of families and communities embedded in different policy environments, and attention to cascades of recessionary shocks, individuals' strategies for coping with them, and the way these intersect with health trajectories.},
	pages = {181--201},
	number = {1},
	journaltitle = {Annual Review of Sociology},
	author = {Burgard, Sarah A. and Kalousova, Lucie},
	urldate = {2016-01-05},
	date = {2015},
	keywords = {Stress, {UNEMPLOYMENT}, foreclosure, health behaviors, inequality, safety net},
	file = {Full Text PDF:C\:\\Users\\bjb40\\Zotero\\storage\\ZRQWEGWJ\\Burgard and Kalousova - 2015 - Effects of the Great Recession Health and Well-Be.pdf:application/pdf}
}

@article{lam_is_2014,
	title = {Is Insecurity Worse for Well-being in Turbulent Times? Mental Health in Context},
	volume = {4},
	issn = {2156-8693, 2156-8731},
	url = {http://smh.sagepub.com/content/4/1/55},
	doi = {10.1177/2156869313507288},
	shorttitle = {Is Insecurity Worse for Well-being in Turbulent Times?},
	abstract = {Using General Social Survey data, we examine whether any association between job insecurity and well-being is contingent on economic climate (comparing those interviewed in turbulent 2010 vs. pre-recessionary 2006), as well as income and gender. We find respondents with higher levels of job insecurity in 2010 reported lower levels of happiness compared to those similarly insecure in 2006. The positive relationship between job insecurity and days of poor mental health becomes more pronounced for those in the third quartile of personal income in 2010, suggesting middle-class vulnerability during the economic downturn. Men (but not women) with higher insecurity report more days of poor mental health in both 2006 and 2010. These findings reinforce a “cycles of control” theoretical approach, given the mental health–job insecurity relationship is heightened for workers in turbulent times.},
	pages = {55--73},
	number = {1},
	journaltitle = {Society and Mental Health},
	shortjournal = {Society and Mental Health},
	author = {Lam, Jack and Fan, Wen and Moen, Phyllis},
	urldate = {2015-01-21},
	date = {2014-03-01},
	langid = {english},
	keywords = {Mental Health, economic strains, gender, social class, work stress},
	file = {Full Text PDF:C\:\\Users\\bjb40\\Zotero\\storage\\MSRC4EMG\\Lam et al. - 2014 - Is Insecurity Worse for Well-being in Turbulent Ti.pdf:application/pdf;Snapshot:C\:\\Users\\bjb40\\Zotero\\storage\\J5XR99KX\\55.html:text/html}
}

@article{ryder_cohort_1965,
	title = {The Cohort as a Concept in the Study of Social Change},
	volume = {30},
	issn = {0003-1224},
	url = {http://www.jstor.org/stable/2090964},
	doi = {10.2307/2090964},
	abstract = {Society persists despite the mortality of its individual members, through processes of demographic metabolism and particularly the annual infusion of birth cohorts. These may pose a threat to stability but they also provide the opportunity for societal transformation. Each birth cohort acquires coherence and continuity from the distinctive development of its constituents and from its own persistent macroanalyic feaures. Successive cohorts are differentiated by the changing content of formal education, by peer-group socialization, and by idiosyncratic historical experience. Young adults are prominent in war, revolution, immigration, urbanization and technological change. Since cohorts are used to achieve structural transformation and since they manifest its consequences in characteristic ways, it is proposed that research be designed to capitalize on the congruence of social change and cohort identification.},
	pages = {843--861},
	number = {6},
	journaltitle = {American Sociological Review},
	shortjournal = {American Sociological Review},
	author = {Ryder, Norman B.},
	urldate = {2016-03-07},
	date = {1965},
	file = {JSTOR Full Text PDF:C\:\\Users\\bjb40\\Zotero\\storage\\KPKA4ZVZ\\Ryder - 1965 - The Cohort as a Concept in the Study of Social Cha.pdf:application/pdf}
}

@incollection{waite_constrained_2014,
	location = {Washington, D.C},
	title = {Constrained Choices: The Shifting Institutional Contexts of Aging and the Life Course},
	isbn = {978-0-309-29297-9},
	url = {http://www.ncbi.nlm.nih.gov/books/NBK179089/},
	pages = {175--216},
	booktitle = {New Directions in the Sociology of Aging},
	publisher = {National Academies Press},
	author = {Moen, Phyllis},
	editor = {Waite, Linda J. and Plewes, Thomas J. and {National Research Council (U.S.)}},
	date = {2014},
	keywords = {Economic aspects, Health aspects, Population aging, {SOCIAL} aspects, United States}
}

@article{nakamura_bayesian_1986,
	title = {Bayesian cohort models for general cohort table analyses},
	volume = {38},
	issn = {0020-3157, 1572-9052},
	url = {http://link.springer.com/article/10.1007/BF02482523},
	doi = {10.1007/BF02482523},
	abstract = {Summary New Bayesian cohort models designed to resolve the identification problem in cohort analysis are proposed in this paper. At first, the basic cohort model which represents the statistical structure of time-series social survey data in terms of age, period and cohort effects is explained. The logit cohort model for qualitative data from a binomial distribution and the normal-type cohort model for quantitative data from a normal distribution are considered as two special cases of the basic model. In order to overcome the identification problem in cohort analysis, a Bayesian approach is adopted, based on the assumption that the effect parameters change gradually. A Bayesian information criterion {ABIC} is introduced for the selection of the optimal model. This approach is so flexible that both the logit and the normal-type cohort models can be made applicable, not only to standard cohort tables but also to general cohort tables in which the range of age group is not equal to the interval between periods. The practical utility of the proposed models is demonstrated by analysing two data sets from the literature on cohort analysis.},
	pages = {353--370},
	number = {1},
	journaltitle = {Annals of the Institute of Statistical Mathematics},
	shortjournal = {Ann Inst Stat Math},
	author = {Nakamura, Takashi},
	urldate = {2016-04-04},
	date = {1986-12},
	langid = {english},
	keywords = {Cohort analysis, Identification problem, Statistics, general, Statistics for Business/Economics/Mathematical Finance/Insurance, Bayesian cohort model, {ABIC}, logit model, general cohort table},
	file = {Full Text PDF:C\:\\Users\\bjb40\\Zotero\\storage\\5MVGPSCH\\Nakamura - 1986 - Bayesian cohort models for general cohort table an.pdf:application/pdf;Snapshot:C\:\\Users\\bjb40\\Zotero\\storage\\JA3CXXSK\\BF02482523.html:text/html}
}

@book{yang_age-period-cohort_2013,
	location = {Boca Raton, {FL}},
	title = {Age-period-cohort analysis: new models, methods, and empirical applications},
	isbn = {978-1-4665-0752-4},
	shorttitle = {Age-period-cohort analysis},
	pagetotal = {338},
	publisher = {{CRC} Press},
	author = {Yang, Yang and Land, Kenneth C.},
	date = {2013},
	keywords = {Age groups, Cohort analysis, Statistical methods}
}

@incollection{mortimer_government_2003,
	location = {New York},
	title = {Government and the Life Course},
	isbn = {0-306-47498-0},
	pages = {205--225},
	booktitle = {Handbook of the life course},
	publisher = {Kluwer Academic/Plenum Publishers,},
	author = {Leisering, Lutz},
	editor = {Mortimer, Jeylan T. and Shanahan, Michael J.},
	date = {2003}
}

@article{vaisey_cultural_2016,
	title = {Cultural Fragmentation or Acquired Dispositions? A New Approach to Accounting for Patterns of Cultural Change},
	volume = {2},
	url = {http://srd.sagepub.com/content/2/2378023116669726.full},
	shorttitle = {Cultural Fragmentation or Acquired Dispositions?},
	pages = {1--15},
	journaltitle = {Socius: Sociological Research for a Dynamic World},
	author = {Vaisey, Stephen and Lizardo, Omar},
	urldate = {2016-11-25},
	date = {2016},
	file = {Vaisey and Lizardo - 2016 - Cultural Fragmentation or Acquired Dispositions A.pdf:C\:\\Users\\bjb40\\Zotero\\storage\\6266GIAS\\Vaisey and Lizardo - 2016 - Cultural Fragmentation or Acquired Dispositions A.pdf:application/pdf}
}

@article{ingle_piecewise_2014,
	title = {Piecewise Linear Slope Estimation},
	volume = {2014},
	issn = {1058-6393},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4516041/},
	doi = {10.1109/ACSSC.2014.7094476},
	abstract = {This paper presents a method for directly estimating slope values in a noisy piecewise linear function. By imposing a Markov structure on the sequence of slopes, piecewise linear fitting is posed as a maximum a posteriori estimation problem. A dynamic program efficiently solves this by traversing a linearly growing trellis. The alternating maximization algorithm (a kind of pseudo-{EM} method) is used to estimate the model parameters from data and its convergence behavior is analyzed. Ultrasound shear wave imaging is presented as a primary application. The algorithm is general enough for applicability in other fields, as suggested by an application to the estimation of shifts in financial interest rate data.},
	pages = {420--422},
	journaltitle = {Conference record / Asilomar Conference on Signals, Systems \& Computers. Asilomar Conference on Signals, Systems \& Computers},
	shortjournal = {Conf Rec Asilomar Conf Signals Syst Comput},
	author = {Ingle, A. N. and Sethares, W. A. and Varghese, T. and Bucklew, J. A.},
	urldate = {2016-12-10},
	date = {2014-11},
	pmid = {26229417},
	pmcid = {PMC4516041},
	file = {PubMed Central Full Text PDF:C\:\\Users\\bjb40\\Zotero\\storage\\CFV38QCJ\\Ingle et al. - 2014 - Piecewise Linear Slope Estimation.pdf:application/pdf}
}

@article{ingle_slope_2015,
	title = {Slope Estimation in Noisy Piecewise Linear Functions},
	volume = {108},
	issn = {0165-1684},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4235795/},
	doi = {10.1016/j.sigpro.2014.10.003},
	abstract = {This paper discusses the development of a slope estimation algorithm called {MAPSlope} for piecewise linear data that is corrupted by Gaussian noise. The number and locations of slope change points (also known as breakpoints) are assumed to be unknown a priori though it is assumed that the possible range of slope values lies within known bounds. A stochastic hidden Markov model that is general enough to encompass real world sources of piecewise linear data is used to model the transitions between slope values and the problem of slope estimation is addressed using a Bayesian maximum a posteriori approach. The set of possible slope values is discretized, enabling the design of a dynamic programming algorithm for posterior density maximization. Numerical simulations are used to justify choice of a reasonable number of quantization levels and also to analyze mean squared error performance of the proposed algorithm. An alternating maximization algorithm is proposed for estimation of unknown model parameters and a convergence result for the method is provided. Finally, results using data from political science, finance and medical imaging applications are presented to demonstrate the practical utility of this procedure.},
	pages = {576--588},
	journaltitle = {Signal processing},
	shortjournal = {Signal Processing},
	author = {Ingle, Atul and Bucklew, James and Sethares, William and Varghese, Tomy},
	urldate = {2016-12-10},
	date = {2015-03-01},
	pmid = {25419020},
	pmcid = {PMC4235795},
	file = {PubMed Central Full Text PDF:C\:\\Users\\bjb40\\Zotero\\storage\\SSEKV25D\\Ingle et al. - 2015 - Slope Estimation in Noisy Piecewise Linear Functio.pdf:application/pdf}
}

@article{denison_automatic_1998,
	title = {Automatic Bayesian Curve Fitting},
	volume = {60},
	issn = {1369-7412},
	url = {http://www.jstor.org/stable/2985943},
	abstract = {A method of estimating a variety of curves by a sequence of piecewise polynomials is proposed, motivated by a Bayesian model and an appropriate summary of the resulting posterior distribution. A joint distribution is set up over both the number and the position of the knots defining the piecewise polynomials. Throughout we use reversible jump Markov chain Monte Carlo methods to compute the posteriors. The methodology has been successful in giving good estimates for `smooth' functions (i.e. continuous and differentiable) as well as functions which are not differentiable, and perhaps not even continuous, at a finite number of points. The methodology is extended to deal with generalized additive models.},
	pages = {333--350},
	number = {2},
	journaltitle = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	shortjournal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
	author = {Denison, D. G. T. and Mallick, B. K. and Smith, A. F. M.},
	urldate = {2016-12-10},
	date = {1998},
	file = {JSTOR Full Text PDF:C\:\\Users\\bjb40\\Zotero\\storage\\RWGUJS4J\\Denison et al. - 1998 - Automatic Bayesian Curve Fitting.pdf:application/pdf}
}

@article{denison_bayesian_1998,
	title = {A Bayesian {CART} algorithm},
	volume = {85},
	issn = {0006-3444, 1464-3510},
	url = {http://biomet.oxfordjournals.org/content/85/2/363},
	doi = {10.1093/biomet/85.2.363},
	abstract = {A stochastic search form of classification and regression tree ({CART}) analysis (Breiman et al., 1984) is proposed, motivated by a Bayesian model. An approximation to a probability distribution over the space of possible trees is explored using reversible jump Markov chain Monte Carlo methods (Green, 1995).},
	pages = {363--377},
	number = {2},
	journaltitle = {Biometrika},
	shortjournal = {Biometrika},
	author = {Denison, David G. T. and Mallick, Bani K. and Smith, Adrian F. M.},
	urldate = {2016-12-10},
	date = {1998-06-01},
	langid = {english},
	keywords = {Bayesian method, Classification tree, Regression tree, Reversible jump Markov chain Monte Carlo},
	file = {Biometrika-1998-DENISON-363-77.pdf:C\:\\Users\\bjb40\\Zotero\\storage\\JNGEU86N\\Biometrika-1998-DENISON-363-77.pdf:application/pdf;Snapshot:C\:\\Users\\bjb40\\Zotero\\storage\\HTUXKZPE\\363.html:text/html}
}

@article{taddy_bayesian_2015,
	title = {Bayesian and empirical Bayesian forests},
	url = {http://arxiv.org/abs/1502.02312},
	journaltitle = {{arXiv} preprint {arXiv}:1502.02312},
	author = {Taddy, Matt and Chen, Chun-Sheng and Yu, Jun and Wyle, Mitch},
	urldate = {2016-12-15},
	date = {2015},
	file = {matthew15.pdf:C\:\\Users\\bjb40\\Zotero\\storage\\EEE8SC32\\matthew15.pdf:application/pdf}
}

@book{breiman_classification_1984,
	location = {New York},
	edition = {1 edition},
	title = {Classification and Regression Trees},
	isbn = {978-0-412-04841-8},
	abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. Both the practical and theoretical sides have been developed in the authors' study of tree methods. Classification and Regression Trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties.},
	pagetotal = {368},
	publisher = {Chapman and Hall/{CRC}},
	author = {Breiman, Leo and Friedman, Jerome and Stone, Charles J. and Olshen, R. A.},
	date = {1984-01-01}
}

@article{breiman_random_2001,
	title = {Random Forests},
	volume = {45},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/article/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	pages = {5--32},
	number = {1},
	journaltitle = {Machine Learning},
	shortjournal = {Machine Learning},
	author = {Breiman, Leo},
	urldate = {2016-12-15},
	date = {2001-10-01},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\bjb40\\Zotero\\storage\\PXB525PT\\Breiman - 2001 - Random Forests.pdf:application/pdf;Snapshot:C\:\\Users\\bjb40\\Zotero\\storage\\UGEQPF9R\\A1010933404324.html:text/html}
}

@article{fragoso_bayesian_2015,
	title = {Bayesian model averaging: A systematic review and conceptual classification},
	url = {http://arxiv.org/abs/1509.08864},
	shorttitle = {Bayesian model averaging},
	journaltitle = {{arXiv} preprint {arXiv}:1509.08864},
	author = {Fragoso, Tiago M. and Neto, Francisco Louzada},
	urldate = {2016-12-15},
	date = {2015},
	file = {1509.08864.pdf:C\:\\Users\\bjb40\\Zotero\\storage\\NBAJMN34\\1509.08864.pdf:application/pdf}
}

@article{raftery_bayesian_1995,
	title = {Bayesian Model Selection in Social Research},
	volume = {25},
	issn = {0081-1750},
	url = {http://www.jstor.org/stable/271063},
	doi = {10.2307/271063},
	abstract = {It is argued that P-values and the tests based upon them give unsatisfactory results, especially in large samples. It is shown that, in regression, when there are many candidate independent variables, standard variable selection procedures can give very misleading results. Also, by selecting a single model, they ignore model uncertainty and so underestimate the uncertainty about quantities of interest. The Bayesian approach to hypothesis testing, model selection, and accounting for model uncertainty is presented. Implementing this is straightforward through the use of the simple and accurate {BIC} approximation, and it can be done using the output from standard software. Specific results are presented for most of the types of model commonly used in sociology. It is shown that this approach overcomes the difficulties with P-values and standard model selection procedures based on them. It also allows easy comparison of nonnested models, and permits the quantification of the evidence for a null hypothesis of interest, such as a convergence theory or a hypothesis about societal norms.},
	pages = {111--163},
	journaltitle = {Sociological Methodology},
	shortjournal = {Sociological Methodology},
	author = {Raftery, Adrian E.},
	urldate = {2016-12-15},
	date = {1995},
	file = {socmeth1995.pdf:C\:\\Users\\bjb40\\Zotero\\storage\\Z635J6UF\\socmeth1995.pdf:application/pdf}
}

@article{mclean_sloughter_probabilistic_2013,
	title = {Probabilistic Wind Vector Forecasting Using Ensembles and Bayesian Model Averaging},
	volume = {141},
	issn = {0027-0644, 1520-0493},
	url = {http://journals.ametsoc.org/doi/abs/10.1175/MWR-D-12-00002.1},
	doi = {10.1175/MWR-D-12-00002.1},
	pages = {2107--2119},
	number = {6},
	journaltitle = {Monthly Weather Review},
	author = {{McLean} Sloughter, J. and Gneiting, Tilmann and Raftery, Adrian E.},
	urldate = {2016-12-15},
	date = {2013-06},
	langid = {english},
	file = {Sloughter2013MWR.pdf:C\:\\Users\\bjb40\\Zotero\\storage\\GH9MXV5U\\Sloughter2013MWR.pdf:application/pdf}
}

@article{gottardo_bayesian_2009,
	title = {Bayesian robust transformation and variable selection: A unified approach},
	volume = {37},
	issn = {1708-945X},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/cjs.10021/abstract},
	doi = {10.1002/cjs.10021},
	shorttitle = {Bayesian robust transformation and variable selection},
	abstract = {The authors consider the problem of simultaneous transformation and variable selection for linear regression. They propose a fully Bayesian solution to the problem, which allows averaging over all models considered including transformations of the response and predictors. The authors use the Box-Cox family of transformations to transform the response and each predictor. To deal with the change of scale induced by the transformations, the authors propose to focus on new quantities rather than the estimated regression coefficients. These quantities, referred to as generalized regression coefficients, have a similar interpretation to the usual regression coefficients on the original scale of the data, but do not depend on the transformations. This allows probabilistic statements about the size of the effect associated with each variable, on the original scale of the data. In addition to variable and transformation selection, there is also uncertainty involved in the identification of outliers in regression. Thus, the authors also propose a more robust model to account for such outliers based on a t-distribution with unknown degrees of freedom. Parameter estimation is carried out using an efficient Markov chain Monte Carlo algorithm, which permits moves around the space of all possible models. Using three real data sets and a simulated study, the authors show that there is considerable uncertainty about variable selection, choice of transformation, and outlier identification, and that there is advantage in dealing with all three simultaneously. The Canadian Journal of Statistics 37: 361–380; 2009 © 2009 Statistical Society of Canada},
	pages = {361--380},
	number = {3},
	journaltitle = {Canadian Journal of Statistics},
	shortjournal = {Can J Statistics},
	author = {Gottardo, Raphael and Raftery, Adrian},
	urldate = {2016-12-15},
	date = {2009-09-01},
	langid = {english},
	keywords = {Markov chain Monte Carlo, Bayesian model averaging, Box-Cox, generalized regression coefficient, linear regression, mixture distribution, t-distribution, {MSC} 2000: Primary 62F15, secondary 62J05},
	file = {Full Text PDF:C\:\\Users\\bjb40\\Zotero\\storage\\3D3G8EPZ\\Gottardo and Raftery - 2009 - Bayesian robust transformation and variable select.pdf:application/pdf;Snapshot:C\:\\Users\\bjb40\\Zotero\\storage\\DT7MAM66\\abstract.html:text/html}
}

@article{eicher_bayesian_2009,
	title = {Bayesian model averaging and endogeneity under model uncertainty: an application to development determinants},
	volume = {94},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.159.6166&rep=rep1&type=pdf},
	shorttitle = {Bayesian model averaging and endogeneity under model uncertainty},
	journaltitle = {Center for Statistics and the Social Sciences University of Washington, Working Paper no},
	author = {Eicher, Theo S. and Lenkoski, Alex and Raftery, Adrian E. and {others}},
	urldate = {2016-12-15},
	date = {2009},
	file = {wp94.pdf:C\:\\Users\\bjb40\\Zotero\\storage\\Q68GH5DR\\wp94.pdf:application/pdf}
}

@article{clyde_bayesian_2011,
	title = {Bayesian Adaptive Sampling for Variable Selection and Model Averaging},
	volume = {20},
	issn = {1061-8600},
	url = {http://dx.doi.org/10.1198/jcgs.2010.09049},
	doi = {10.1198/jcgs.2010.09049},
	abstract = {For the problem of model choice in linear regression, we introduce a Bayesian adaptive sampling algorithm ({BAS}), that samples models without replacement from the space of models. For problems that permit enumeration of all models, {BAS} is guaranteed to enumerate the model space in 2p iterations where p is the number of potential variables under consideration. For larger problems where sampling is required, we provide conditions under which {BAS} provides perfect samples without replacement. When the sampling probabilities in the algorithm are the marginal variable inclusion probabilities, {BAS} may be viewed as sampling models “near” the median probability model of Barbieri and Berger. As marginal inclusion probabilities are not known in advance, we discuss several strategies to estimate adaptively the marginal inclusion probabilities within {BAS}. We illustrate the performance of the algorithm using simulated and real data and show that {BAS} can outperform Markov chain Monte Carlo methods. The algorithm is implemented in the R package {BAS} available at {CRAN}. This article has supplementary material online.},
	pages = {80--101},
	number = {1},
	journaltitle = {Journal of Computational and Graphical Statistics},
	author = {Clyde, Merlise A. and Ghosh, Joyee and Littman, Michael L.},
	urldate = {2016-12-15},
	date = {2011-01-01},
	keywords = {Markov chain Monte Carlo, Bayesian model averaging, Inclusion probability, Median probability model, Model uncertainty, Sampling without replacement},
	file = {Full Text PDF:C\:\\Users\\bjb40\\Zotero\\storage\\ZGNF3UK7\\Clyde et al. - 2011 - Bayesian Adaptive Sampling for Variable Selection .pdf:application/pdf;Snapshot:C\:\\Users\\bjb40\\Zotero\\storage\\NB8HZJRX\\jcgs.2010.html:text/html}
}

@article{hernandez_bayesian_2015,
	title = {Bayesian Additive Regression Trees using Bayesian Model Averaging},
	url = {http://arxiv.org/abs/1507.00181},
	abstract = {Bayesian Additive Regression Trees ({BART}) is a statistical sum of trees model. It can be considered a Bayesian version of machine learning tree ensemble methods where the individual trees are the base learners. However for data sets where the number of variables \$p\$ is large (e.g. \$p{\textgreater}5,000\$) the algorithm can become prohibitively expensive, computationally. Another method which is popular for high dimensional data is random forests, a machine learning algorithm which grows trees using a greedy search for the best split points. However, as it is not a statistical model, it cannot produce probabilistic estimates or predictions. We propose an alternative algorithm for {BART} called {BART}-{BMA}, which uses Bayesian Model Averaging and a greedy search algorithm to produce a model which is much more efficient than {BART} for datasets with large \$p\$. {BART}-{BMA} incorporates elements of both {BART} and random forests to offer a model-based algorithm which can deal with high-dimensional data. We have found that {BART}-{BMA} can be run in a reasonable time on a standard laptop for the "small \$n\$ large \$p\$" scenario which is common in many areas of bioinformatics. We showcase this method using simulated data and data from two real proteomic experiments; one to distinguish between patients with cardiovascular disease and controls and another to classify agressive from non-agressive prostate cancer. We compare our results to their main competitors. Open source code written in R and Rcpp to run {BART}-{BMA} can be found at: https://github.com/{BelindaHernandez}/{BART}-{BMA}.git},
	journaltitle = {{arXiv}:1507.00181 [stat]},
	author = {Hernández, Belinda and Raftery, Adrian E. and Pennington, Stephen R. and Parnell, Andrew C.},
	urldate = {2016-12-15},
	date = {2015-07-01},
	eprinttype = {arxiv},
	eprint = {1507.00181},
	keywords = {Statistics - Computation, Statistics - Methodology},
	file = {arXiv\:1507.00181 PDF:C\:\\Users\\bjb40\\Zotero\\storage\\PGS4PXJT\\Hernández et al. - 2015 - Bayesian Additive Regression Trees using Bayesian .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bjb40\\Zotero\\storage\\KAA6GI6J\\1507.html:text/html}
}

@article{raftery_using_2005,
	title = {Using Bayesian model averaging to calibrate forecast ensembles},
	volume = {133},
	url = {http://journals.ametsoc.org/doi/abs/10.1175/MWR2906.1},
	pages = {1155--1174},
	number = {5},
	journaltitle = {Monthly Weather Review},
	author = {Raftery, Adrian E. and Gneiting, Tilmann and Balabdaoui, Fadoua and Polakowski, Michael},
	urldate = {2016-12-15},
	date = {2005},
	file = {fadoua2005(1).pdf:C\:\\Users\\bjb40\\Zotero\\storage\\M2TSQ9IM\\fadoua2005(1).pdf:application/pdf}
}

@article{raftery_using_2005-1,
	title = {Using Bayesian Model Averaging to Calibrate Forecast Ensembles},
	volume = {133},
	issn = {0027-0644},
	url = {http://journals.ametsoc.org/doi/abs/10.1175/MWR2906.1},
	doi = {10.1175/MWR2906.1},
	abstract = {Ensembles used for probabilistic weather forecasting often exhibit a spread-error correlation, but they tend to be underdispersive. This paper proposes a statistical method for postprocessing ensembles based on Bayesian model averaging ({BMA}), which is a standard method for combining predictive distributions from different sources. The {BMA} predictive probability density function ({PDF}) of any quantity of interest is a weighted average of {PDFs} centered on the individual bias-corrected forecasts, where the weights are equal to posterior probabilities of the models generating the forecasts and reflect the models' relative contributions to predictive skill over the training period. The {BMA} weights can be used to assess the usefulness of ensemble members, and this can be used as a basis for selecting ensemble members; this can be useful given the cost of running large ensembles. The {BMA} {PDF} can be represented as an unweighted ensemble of any desired size, by simulating from the {BMA} predictive distribution. The {BMA} predictive variance can be decomposed into two components, one corresponding to the between-forecast variability, and the second to the within-forecast variability. Predictive {PDFs} or intervals based solely on the ensemble spread incorporate the first component but not the second. Thus {BMA} provides a theoretical explanation of the tendency of ensembles to exhibit a spread-error correlation but yet be underdispersive. The method was applied to 48-h forecasts of surface temperature in the Pacific Northwest in January–June 2000 using the University of Washington fifth-generation Pennsylvania State University–{NCAR} Mesoscale Model ({MM}5) ensemble. The predictive {PDFs} were much better calibrated than the raw ensemble, and the {BMA} forecasts were sharp in that 90\% {BMA} prediction intervals were 66\% shorter on average than those produced by sample climatology. As a by-product, {BMA} yields a deterministic point forecast, and this had root-mean-square errors 7\% lower than the best of the ensemble members and 8\% lower than the ensemble mean. Similar results were obtained for forecasts of sea level pressure. Simulation experiments show that {BMA} performs reasonably well when the underlying ensemble is calibrated, or even overdispersed.},
	pages = {1155--1174},
	number = {5},
	journaltitle = {Monthly Weather Review},
	shortjournal = {Mon. Wea. Rev.},
	author = {Raftery, Adrian E. and Gneiting, Tilmann and Balabdaoui, Fadoua and Polakowski, Michael},
	urldate = {2016-12-15},
	date = {2005-05-01},
	file = {Full Text PDF:C\:\\Users\\bjb40\\Zotero\\storage\\7IXVXTDU\\Raftery et al. - 2005 - Using Bayesian Model Averaging to Calibrate Foreca.pdf:application/pdf;Snapshot:C\:\\Users\\bjb40\\Zotero\\storage\\AEMHRCFB\\MWR2906.html:text/html}
}

@article{jackson_biological_2003,
	title = {Biological age—what is it and can it be measured?},
	volume = {36},
	url = {http://www.sciencedirect.com/science/article/pii/S0167494302000602},
	pages = {103--115},
	number = {2},
	journaltitle = {Archives of gerontology and geriatrics},
	author = {Jackson, Stephen {HD} and Weale, Martin R. and Weale, Robert A.},
	urldate = {2017-06-20},
	date = {2003},
	file = {Biological-age-what-is-it-and-can-it-be-measured-_2003_Archives-of-Gerontology-and-Geriatrics.pdf:C\:\\Users\\bjb40\\Zotero\\storage\\5DV6SCKC\\Biological-age-what-is-it-and-can-it-be-measured-_2003_Archives-of-Gerontology-and-Geriatrics.pdf:application/pdf}
}

@article{luo_block_2016,
	title = {Block Constraints in Age–Period–Cohort Models with Unequal-width Intervals},
	volume = {45},
	issn = {0049-1241},
	url = {http://journals.sagepub.com/doi/abs/10.1177/0049124115585359},
	doi = {10.1177/0049124115585359},
	abstract = {Age–period–cohort ({APC}) models are designed to estimate the independent effects of age, time periods, and cohort membership. However, {APC} models suffer from an identification problem: There are no unique estimates of the independent effects that fit the data best because of the exact linear dependency among age, period, and cohort. Among methods proposed to address this problem, using unequal-interval widths for age, period, and cohort categories appears to break the exact linear dependency and thus solve the identification problem. However, this article shows that the identification problem remains in these models; in fact, they just implicitly impose multiple block constraints on the age, period, and cohort effects to achieve identifiability. These constraints depend on an arbitrary choice of widths for the age, period, and cohort intervals and can have nontrivial effects on the estimates. Because these assumptions are extremely difficult, if not impossible, to verify in empirical research, they are qualitatively no different from the assumptions of other constrained estimators. Therefore, the unequal-intervals approach should not be used without an explicit rationale justifying their constraints.},
	pages = {700--726},
	number = {4},
	journaltitle = {Sociological Methods \& Research},
	shortjournal = {Sociological Methods \& Research},
	author = {Luo, Liying and Hodges, James S.},
	urldate = {2017-06-22},
	date = {2016-11-01},
	langid = {english},
	file = {SAGE PDF Full Text:C\:\\Users\\bjb40\\Zotero\\storage\\4FP3S87N\\Luo and Hodges - 2016 - Block Constraints in Age–Period–Cohort Models with.pdf:application/pdf}
}

@article{hoeting_bayesian_1999,
	title = {Bayesian model averaging: a tutorial},
	url = {http://www.jstor.org/stable/2676803},
	shorttitle = {Bayesian model averaging},
	pages = {382--401},
	journaltitle = {Statistical science},
	author = {Hoeting, Jennifer A. and Madigan, David and Raftery, Adrian E. and Volinsky, Chris T.},
	urldate = {2017-06-23},
	date = {1999},
	file = {hoeting1999(1).pdf:C\:\\Users\\bjb40\\Zotero\\storage\\NKGU6D6H\\hoeting1999(1).pdf:application/pdf}
}

@article{hoeting_bayesian_2002,
	title = {Bayesian variable and transformation selection in linear regression},
	volume = {11},
	url = {http://amstat.tandfonline.com/doi/abs/10.1198/106186002501},
	pages = {485--507},
	number = {3},
	journaltitle = {Journal of Computational and Graphical Statistics},
	author = {Hoeting, Jennifer A. and Raftery, Adrian E. and Madigan, David},
	urldate = {2017-06-23},
	date = {2002},
	file = {hoeting2002.pdf:C\:\\Users\\bjb40\\Zotero\\storage\\EQGS5PD7\\hoeting2002.pdf:application/pdf}
}

@article{forte_methods_2016,
	title = {Methods and Tools for Bayesian Variable Selection and Model Averaging in Univariate Linear Regression},
	url = {http://arxiv.org/abs/1612.02357},
	abstract = {In this paper we briefly review the main methodological aspects concerned with the application of the Bayesian approach to model choice and model averaging in the context of variable selection in regression models. This includes prior elicitation, summaries of the posterior distribution and computational strategies. We then examine and compare various publicly available \{{\textbackslash}tt R\}-packages for its practical implementation summarizing and explaining the differences between packages and giving recommendations for applied users. We find that all packages reviewed lead to very similar results, but there are potentially important differences in flexibility and efficiency of the packages.},
	journaltitle = {{arXiv}:1612.02357 [stat]},
	author = {Forte, Anabel and Garcia-Donato, Gonzalo and Steel, Mark},
	urldate = {2017-06-23},
	date = {2016-12-07},
	eprinttype = {arxiv},
	eprint = {1612.02357},
	keywords = {Statistics - Computation},
	file = {arXiv\:1612.02357 PDF:C\:\\Users\\bjb40\\Zotero\\storage\\V476W33V\\Forte et al. - 2016 - Methods and Tools for Bayesian Variable Selection .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\bjb40\\Zotero\\storage\\AXUTQX43\\1612.html:text/html}
}

@article{raftery_bayesian_1997,
	title = {Bayesian Model Averaging for Linear Regression Models},
	volume = {92},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1080/01621459.1997.10473615},
	doi = {10.1080/01621459.1997.10473615},
	abstract = {We consider the problem of accounting for model uncertainty in linear regression models. Conditioning on a single selected model ignores model uncertainty, and thus leads to the underestimation of uncertainty when making inferences about quantities of interest. A Bayesian solution to this problem involves averaging over all possible models (i.e., combinations of predictors) when making inferences about quantities of interest. This approach is often not practical. In this article we offer two alternative approaches. First, we describe an ad hoc procedure, “Occam's window,” which indicates a small set of models over which a model average can be computed. Second, we describe a Markov chain Monte Carlo approach that directly approximates the exact solution. In the presence of model uncertainty, both of these model averaging procedures provide better predictive performance than any single model that might reasonably have been selected. In the extreme case where there are many candidate predictors but no relationship between any of them and the response, standard variable selection procedures often choose some subset of variables that yields a high R 2 and a highly significant overall F value. In this situation, Occam's window usually indicates the null model (or a small number of models including the null model) as the only one (or ones) to be considered thus largely resolving the problem of selecting significant models when there is no signal in the data. Software to implement our methods is available from {StatLib}.},
	pages = {179--191},
	number = {437},
	journaltitle = {Journal of the American Statistical Association},
	author = {Raftery, Adrian E. and Madigan, David and Hoeting, Jennifer A.},
	urldate = {2017-06-28},
	date = {1997-03-01},
	keywords = {Bayes factor, Model uncertainty, Markov chain Monte Carlo model composition, Occam's window, Posterior model probability},
	file = {Snapshot:C\:\\Users\\bjb40\\Zotero\\storage\\V8XEJWIJ\\01621459.1997.html:text/html}
}

@article{luo_assessing_2013,
	title = {Assessing Validity and Application Scope of the Intrinsic Estimator Approach to the Age-Period-Cohort Problem},
	volume = {50},
	issn = {0070-3370, 1533-7790},
	url = {http://link.springer.com/10.1007/s13524-013-0243-z},
	doi = {10.1007/s13524-013-0243-z},
	pages = {1945--1967},
	number = {6},
	journaltitle = {Demography},
	author = {Luo, Liying},
	urldate = {2017-07-03},
	date = {2013-12},
	langid = {english}
}

@article{bell_hierarchical_2017,
	title = {The hierarchical age-period-cohort model: Why does it find the results that it finds?},
	issn = {0033-5177, 1573-7845},
	url = {http://link.springer.com/10.1007/s11135-017-0488-5},
	doi = {10.1007/s11135-017-0488-5},
	shorttitle = {The hierarchical age?},
	journaltitle = {Quality \& Quantity},
	author = {Bell, Andrew and Jones, Kelvyn},
	urldate = {2017-07-03},
	date = {2017-02-24},
	langid = {english}
}

@incollection{glenn_strategies_2005,
	location = {2455 Teller Road,?Thousand Oaks?California?91320?United States of America?},
	title = {Strategies for Estimating Age, Period, and Cohort Effects},
	isbn = {978-0-7619-2215-5 978-1-4129-8366-2},
	url = {http://methods.sagepub.com/book/cohort-analysis/n2.xml},
	pages = {12--35},
	booktitle = {Cohort Analysis},
	publisher = {{SAGE} Publications, Inc.},
	author = {Glenn, Norval},
	urldate = {2017-07-03},
	date = {2005},
	doi = {10.4135/9781412983662.n2}
}

@article{seungdamrong_relative_1975,
	title = {Relative severity of hemolysis in the subtypes of {ABO} incompatible hemolytic disease},
	volume = {148},
	pages = {526--527, 530},
	number = {5},
	journaltitle = {{IMJ}. Illinois medical journal},
	shortjournal = {{IMJ} Ill Med J},
	author = {Seungdamrong, S. and Yasunaga, S.},
	date = {1975-11},
	pmid = {305},
	keywords = {Female, Humans, Pregnancy, {ABO} Blood-Group System, Erythroblastosis, Fetal, Hemolysis, Infant, Newborn}
}

@article{makar_formate_1975,
	title = {Formate assay in body fluids: application in methanol poisoning},
	volume = {13},
	issn = {0006-2944},
	shorttitle = {Formate assay in body fluids},
	pages = {117--126},
	number = {2},
	journaltitle = {Biochemical Medicine},
	shortjournal = {Biochem Med},
	author = {Makar, A. B. and {McMartin}, K. E. and Palese, M. and Tephly, T. R.},
	date = {1975-06},
	pmid = {1},
	keywords = {Aldehyde Oxidoreductases, Animals, Body Fluids, Carbon Dioxide, Formates, Haplorhini, Humans, Hydrogen-Ion Concentration, Kinetics, Methanol, Pseudomonas, methods}
}

@article{boozer_survey_1976,
	title = {Survey of curriculum and instruction: oral diagnosis},
	volume = {40},
	issn = {0022-0337},
	shorttitle = {Survey of curriculum and instruction},
	pages = {800--804},
	number = {12},
	journaltitle = {Journal of Dental Education},
	shortjournal = {J Dent Educ},
	author = {Boozer, C. H. and Rasmussen, R. H. and Barrett, R. A.},
	date = {1976-12},
	pmid = {62773},
	keywords = {Canada, United States, Audiovisual Aids, Curriculum, Dental Records, Diagnosis, Oral, Education, Dental, Education, Dental, Continuing, Licensure, Dental, Organization and Administration, Patient Care Planning, Teaching}
}

@book{camm_essentials_2016,
	location = {Mason, {OH}},
	title = {Essentials of business analytics},
	isbn = {978-1-305-62773-4},
	publisher = {Cengage South western},
	author = {Camm, Jeffrey D.},
	date = {2016}
}

@book{wooldridge_introductory_2009,
	location = {Mason, {OH}},
	edition = {4th ed},
	title = {Introductory econometrics: a modern approach},
	isbn = {978-0-324-58162-1 978-0-324-66060-9 978-0-324-66054-8},
	shorttitle = {Introductory econometrics},
	pagetotal = {865},
	publisher = {South Western, Cengage Learning},
	author = {Wooldridge, Jeffrey M.},
	date = {2009},
	keywords = {Econometrics}
}

@article{madigan_bayesian_1996,
	title = {Bayesian model averaging and model selection for markov equivalence classes of acyclic digraphs},
	volume = {25},
	issn = {0361-0926},
	url = {http://dx.doi.org/10.1080/03610929608831853},
	doi = {10.1080/03610929608831853},
	abstract = {Acyclic digraphs ({ADGs}) are widely used to describe dependences among variables in multivariate distributions. In particular, the likelihood functions of {ADG} models admit convenient recursive factorizations that often allow explicit maximum likelihood estimates and that are well suited to building Bayesian networks for expert systems. There may, however, be many {ADGs} that determine the same dependence (= Markov) model. Thus, the family of all {ADGs} with a given set of vertices is naturally partitioned into Markov-equivalence classes, each class being associated with a unique statistical model. Statistical procedures, such as model selection or model averaging, that fail to take into account these equivalence classes, may incur substantial computational or other inefficiencies. Recent results have shown that each Markov-equivalence class is uniquely determined by a single chain graph, the essential graph, that is itself Markov-equivalent simultaneously to all {ADGs} in the equivalence class. Here we propose two stochastic Bayesian model averaging and selection algorithms for essential graphs and apply them to the analysis of three discrete-variable data sets.},
	pages = {2493--2519},
	number = {11},
	journaltitle = {Communications in Statistics - Theory and Methods},
	author = {Madigan, David and Andersson, Steen A. and Perlman, Michael D. and Volinsky, Chris T.},
	urldate = {2017-07-10},
	date = {1996-01-01},
	keywords = {Markov chain Monte Carlo, Model uncertainty, Bayesian graphical model, Essential graph, model averaging, Markov equivalence},
	file = {Snapshot:C\:\\Users\\bjb40\\Zotero\\storage\\X2DASN8E\\03610929608831853.html:text/html}
}

@article{fligstein_seeing_2017,
	title = {Seeing Like the Fed: Culture, Cognition, and Framing in the Failure to Anticipate the Financial Crisis of 2008},
	volume = {82},
	issn = {0003-1224},
	url = {https://doi.org/10.1177/0003122417728240},
	doi = {10.1177/0003122417728240},
	shorttitle = {Seeing Like the Fed},
	abstract = {One of the puzzles about the financial crisis of 2008 is why regulators, particularly the Federal Open Market Committee ({FOMC}), were so slow to recognize the impending collapse of the financial system and its broader consequences for the economy. We use theory from the literature on culture, cognition, and framing to explain this puzzle. Consistent with recent work on “positive asymmetry,” we show how the {FOMC} generally interpreted discomforting facts in a positive light, marginalizing and normalizing anomalous information. We argue that all frames limit what can be understood, but the content of frames matters for how facts are identified and explained. We provide evidence that the Federal Reserve’s primary frame for making sense of the economy was macroeconomic theory. The content of macroeconomics made it difficult for the {FOMC} to connect events into a narrative reflecting the links between foreclosures in the housing market, the financial instruments used to package the mortgages into securities, and the threats to the larger economy. We conclude with implications for the sociological literatures on framing and cognition and for decision-making in future crises.},
	pages = {879--909},
	number = {5},
	journaltitle = {American Sociological Review},
	shortjournal = {Am Sociol Rev},
	author = {Fligstein, Neil and Brundage, Jonah Stuart and Schultz, Michael},
	date = {2017-10-01},
	langid = {english},
	file = {SAGE PDF Full Text:C\:\\Users\\bjb40\\Zotero\\storage\\Q936ETKA\\Fligstein et al. - 2017 - Seeing Like the Fed Culture, Cognition, and Frami.pdf:application/pdf}
}

@book{hardy_regression_1993,
	location = {Newbury Park},
	title = {Regression with dummy variables},
	isbn = {978-0-8039-5128-0},
	series = {Sage university papers series},
	pagetotal = {90},
	number = {no. 07-093},
	publisher = {Sage Publications},
	author = {Hardy, Melissa A.},
	date = {1993},
	keywords = {Dummy variables, Regression analysis, Social sciences, Statistical methods}
}